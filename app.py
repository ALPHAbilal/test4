# app.py - Implementing OpenAI Agents SDK Workflow (Vector Store Only)

import os
import json
import asyncio
import logging
import time
import uuid
import hashlib
# Import PDF handling utilities
from tools.pymupdf_tools import extract_text_from_pdf
import traceback
import re
import markdown
import html
import ast
from dotenv import load_dotenv, find_dotenv

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO if os.getenv('FLASK_DEBUG') != '1' else logging.DEBUG,
    format='%(asctime)s [%(levelname)s] %(name)s:%(lineno)d: %(message)s'
)
logger = logging.getLogger(__name__)
from flask import Flask, render_template, request, flash, redirect, url_for, session, jsonify, send_file
from werkzeug.utils import secure_filename
from openai import (
    OpenAI, APIConnectionError, AuthenticationError, RateLimitError,
    NotFoundError, BadRequestError, APIStatusError
)
from typing import Optional, List, Dict, Any, Union

# Import semantic cache for agent-driven query caching
from data.semantic_cache import semantic_search_cache

# --- Agent SDK Imports ---
from agents import Agent, Runner, Handoff, RunContextWrapper, function_tool, AgentOutputSchema
from agents.result import RunResult  # Import for extract_final_answer function
from agents.agent_output import AgentOutputSchema as AgentOutputSchemaClass  # Import for custom Runner
from agents import items as agents_items  # Import for ToolCallItem and ToolCallOutputItem
# --- CORRECTED & CONFIRMED Tracing Imports ---
from agents.tracing.processor_interface import TracingProcessor # Correct base class path
from agents.tracing.traces import Trace # Correct type hint path
from agents.tracing.spans import Span # Needed for type hints in processor methods
from agents.tracing import add_trace_processor # Correct registration function path
# --- END CORRECTED Imports ---

# --- Agent Registry Imports ---
try:
    # Try importing from the updated registry structure
    from agents.registry.registry_loader import initialize_agent_registry
    logger.info("Using new agents registry structure")
except ImportError:
    # Fallback to the old registry
    from agents.registry import AgentRegistry, initialize_agent_registry
    logger.info("Using old agents registry structure")

# Define retrieve_template_content function here to avoid circular imports
# Import RunContextWrapper if not already imported
from agents import RunContextWrapper

# Define the RetrievalSuccess and RetrievalError classes if they're not already defined
class RetrievalSuccess:
    def __init__(self, content, source_filename, sources=None):
        self.content = content
        self.source_filename = source_filename
        self.sources = sources or []

class RetrievalError:
    def __init__(self, error_message, details=None, query_attempted=None):
        self.error_message = error_message
        self.details = details
        self.query_attempted = query_attempted

@function_tool(strict_mode=False)
async def retrieve_template_content(ctx: RunContextWrapper, template_name: str) -> Union[RetrievalSuccess, RetrievalError]:
    """Retrieves the text content of a specified document template (txt, md, pdf)."""
    logger.info(f"[Tool Call] retrieve_template_content: template_name='{template_name}'")
    try:
        # Use asyncio.to_thread to run the file operations asynchronously
        async def process_template():
            # Sanitize template_name but preserve extension
            original_filename = secure_filename(template_name)
            base_name, ext = os.path.splitext(original_filename)
            if not ext: ext = ".md" # Default to markdown if no extension provided

            # Check if extension is allowed for templates
            allowed_template_exts = {'.txt', '.md', '.pdf'}
            if ext.lower() not in allowed_template_exts:
                logger.error(f"Attempted to retrieve template with unsupported extension: {original_filename}")
                return RetrievalError(error_message=f"Unsupported template file type '{ext}'. Allowed: {', '.join(allowed_template_exts)}")

            final_filename = f"{base_name}{ext}" # Reconstruct potentially sanitized name
            template_path = os.path.join(TEMPLATE_DIR, final_filename)

            # Security check: Ensure path is still within TEMPLATE_DIR
            if not os.path.exists(template_path) or os.path.commonpath([TEMPLATE_DIR]) != os.path.commonpath([TEMPLATE_DIR, template_path]):
                logger.error(f"[Tool Error] Template file not found or invalid path: {template_path}")
                return RetrievalError(error_message=f"Template '{template_name}' not found.")

            # --- Extract content based on type ---
            content = ""
            logger.info(f"Reading template file: {template_path}")
            if ext.lower() == ".pdf":
                # Use our robust PDF text extraction function
                content = extract_text_from_pdf(template_path)
            elif ext.lower() in [".txt", ".md"]:
                with open(template_path, 'r', encoding='utf-8') as f:
                    content = f.read()

            if not content:
                logger.warning(f"Extracted empty content from template: {final_filename}")
                return RetrievalError(error_message=f"Could not extract content from template '{final_filename}'.")

            logger.info(f"[Tool Result] Retrieved template '{final_filename}'. Length: {len(content)}")
            cleaned_content = re.sub(r'\s+', ' ', content).strip()
            return RetrievalSuccess(content=cleaned_content, source_filename=f"Template: {original_filename}")

        # Run the template processing function asynchronously
        return await process_template()

    except Exception as e:
         logger.error(f"[Tool Error] Error retrieving template '{template_name}': {e}", exc_info=True)
         return RetrievalError(error_message=f"Error retrieving template: {str(e)}")

# Ensure ContentProcessorAgent is available
from agents import ContentProcessorAgent

# --- Tool Registry Imports ---
from tools.registry import ToolRegistry, initialize_tool_registry

# --- Workflow Registry Imports ---
from core.workflow_registry import WorkflowRegistry, workflow_registry, initialize_workflow_registry

# --- Orchestration Engine Imports ---
from core.orchestration import OrchestrationEngine, execute_orchestrated_workflow

# --- Enhanced Intent Determination ---
from core.intent_determination import determine_final_intent, record_intent_determination

# --- DocumentAnalyzerAgent Integration ---
# Import the integration module (functions will be imported later)
import core.document_analyzer_integration as document_analyzer_integration

# Import tools from document_analyzer_agent
from agents.document_analyzer_agent import detect_fields_from_template, analyze_document_for_workflow
from data.data_models import DocumentAnalysis

# --- Pydantic Models ---
from pydantic import BaseModel, Field, ConfigDict

# --- Load Configuration ---
load_dotenv(find_dotenv(), override=True)

# --- App Setup ---
app = Flask(__name__)
app.config['SECRET_KEY'] = os.getenv('FLASK_SECRET_KEY')
if not app.config['SECRET_KEY']:
    logger.warning("FLASK_SECRET_KEY not set. Using temporary key.")
    app.config['SECRET_KEY'] = os.urandom(24)

# --- Initialize Agent Registry ---
try:
    agent_registry = initialize_agent_registry()
    logger.info(f"Agent registry initialized with {len(agent_registry.get_all_agents()) if hasattr(agent_registry, 'get_all_agents') else len(agent_registry.agents) if hasattr(agent_registry, 'agents') else '?'} agents")
except Exception as e:
    logger.error(f"Error initializing agent registry: {e}")
    agent_registry = None

# Ensure critical agents are registered
if agent_registry:
    # Check if ContentProcessorAgent is in the registry
    if not agent_registry.get_agent("ContentProcessorAgent"):
        logger.warning("ContentProcessorAgent not found in registry, adding manually")
        agent_registry.agents["ContentProcessorAgent"] = ContentProcessorAgent

    # Check if WorkflowRouterAgent is in the registry
    if not agent_registry.get_agent("WorkflowRouterAgent"):
        logger.warning("WorkflowRouterAgent not found in registry, adding manually")
        from agents import Agent
        workflow_router_agent = Agent(
            name="WorkflowRouterAgent",
            instructions="""You are a workflow orchestration agent responsible for determining the next step in processing a user query. Your job is to analyze the user's query, available templates, temporary files, conversation history, and the current state of the workflow to decide what action should be taken next.""",
            model="gpt-4o-mini"
        )
        agent_registry.agents["WorkflowRouterAgent"] = workflow_router_agent
        logger.info("Manually created and registered WorkflowRouterAgent")

    # Check if QueryAnalyzerAgent is in the registry
    if not agent_registry.get_agent("QueryAnalyzerAgent"):
        logger.warning("QueryAnalyzerAgent not found in registry, adding manually")
        from agents import Agent
        query_analyzer_agent = Agent(
            name="QueryAnalyzerAgent",
            instructions="""Analyze the user's query, available templates, and temporary files to determine the true intent with high accuracy.""",
            model="gpt-4o-mini"
        )
        agent_registry.agents["QueryAnalyzerAgent"] = query_analyzer_agent
        logger.info("Manually created and registered QueryAnalyzerAgent")

    # Check if DataGatheringAgentMinimal is in the registry
    if not agent_registry.get_agent("DataGatheringAgentMinimal"):
        logger.warning("DataGatheringAgentMinimal not found in registry, adding manually")
        from agents import Agent
        data_gathering_agent = Agent(
            name="DataGatheringAgentMinimal",
            instructions="""You are a specialized data gathering agent. Your job is to retrieve relevant information based on the user's query and intent.""",
            model="gpt-4o-mini"
        )
        agent_registry.agents["DataGatheringAgentMinimal"] = data_gathering_agent
        logger.info("Manually created and registered DataGatheringAgentMinimal")

# --- Initialize Tool Registry ---
try:
    tool_registry = initialize_tool_registry()
    logger.info(f"Tool registry initialized with {len(tool_registry.tools)} tools")
except Exception as e:
    logger.error(f"Error initializing tool registry: {e}")
    tool_registry = None

# --- Initialize Workflow Registry ---
try:
    workflow_registry = initialize_workflow_registry()
    logger.info(f"Workflow registry initialized with {len(workflow_registry.workflows)} workflows")
except Exception as e:
    logger.error(f"Error initializing workflow registry: {e}")
    workflow_registry = None

# --- Constants & Configurable Values ---
UPLOAD_FOLDER = os.getenv('UPLOAD_FOLDER', 'uploads')
TEMPLATE_DIR = os.getenv('TEMPLATE_DIR', 'templates/document_templates')
DOCX_OUTPUT_DIR = os.getenv('DOCX_OUTPUT_DIR', 'docx_output')
# Ensure the path is absolute
if not os.path.isabs(DOCX_OUTPUT_DIR):
    DOCX_OUTPUT_DIR = os.path.abspath(DOCX_OUTPUT_DIR)
ALLOWED_EXTENSIONS = {'pdf'}
ALLOWED_TEMPLATE_EXTENSIONS = {'txt', 'md', 'pdf'}
DATABASE_FILE = os.getenv('DATABASE_FILE', 'chat_history.db')
COMPLETION_MODEL = os.getenv('COMPLETION_MODEL', 'gpt-4o-mini')
MAX_COMPLETION_TOKENS = int(os.getenv('MAX_COMPLETION_TOKENS', 8000))
COMPLETION_TEMPERATURE = float(os.getenv('COMPLETION_TEMPERATURE', 0.3))
MAX_SEARCH_RESULTS_TOOL = int(os.getenv('MAX_SEARCH_RESULTS_TOOL', 5)) # Chunks per tool call
SEARCH_RANKER = os.getenv('SEARCH_RANKER', 'auto')
DEFAULT_VS_CACHE_DURATION = int(os.getenv('DEFAULT_VS_CACHE_DURATION', 300))

# Ensure output directories exist
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(DOCX_OUTPUT_DIR, exist_ok=True)

# --- Setup Flask App ---

# Ensure template directory exists
os.makedirs(TEMPLATE_DIR, exist_ok=True)

# --- Database Setup ---
try:
    from data.chat_db import ChatHistoryDB
    chat_db = ChatHistoryDB(DATABASE_FILE)
    logger.info(f"ChatHistoryDB initialized with {DATABASE_FILE}")
except Exception as db_init_err:
    logger.error(f"Failed to initialize ChatHistoryDB: {db_init_err}", exc_info=True)
    chat_db = None

# --- OpenAI Client Setup ---
client: Optional[OpenAI] = None
def get_openai_client() -> Optional[OpenAI]:
    global client
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key: logger.error("OPENAI_API_KEY not found."); client = None; return None
    if client is None or client.api_key != api_key:
        try: client = OpenAI(api_key=api_key, timeout=45.0); logger.info("OpenAI client (re)initialized.")
        except Exception as e: logger.error(f"Failed to init OpenAI client: {e}", exc_info=True); client = None; return None
    return client

def get_model_with_fallback(preferred_model=COMPLETION_MODEL):
    """Get the preferred model with fallback to a more stable model if needed."""
    # Allow override via environment variable
    model = os.getenv("OPENAI_MODEL", preferred_model)

    # Define fallback chain
    fallback_models = ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"]

    # If the preferred model is in the fallback chain, start from there
    if model in fallback_models:
        fallback_index = fallback_models.index(model)
        fallback_models = fallback_models[fallback_index:]

    # Return the preferred model and the fallback chain
    return model, fallback_models

# --- Helper Functions ---
def allowed_file(filename): return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# --- Cache Management Functions ---
async def clear_semantic_cache():
    """Clear the semantic search cache"""
    semantic_search_cache.cache.clear()
    semantic_search_cache.timestamps.clear()
    logger.info("Semantic search cache cleared")

async def get_cache_stats():
    """Get statistics about the semantic cache"""
    total_entries = len(semantic_search_cache.cache)
    expired_entries = sum(1 for key, timestamp in semantic_search_cache.timestamps.items()
                         if time.time() - timestamp > semantic_search_cache.ttl)
    return {
        "total_entries": total_entries,
        "active_entries": total_entries - expired_entries,
        "expired_entries": expired_entries,
        "cache_size_limit": semantic_search_cache.max_size,
        "ttl_seconds": semantic_search_cache.ttl,
        "confidence_threshold": semantic_search_cache.confidence_threshold
    }

# --- Vector Store Functions ---
vector_store_cache = {"list": [], "last_updated": 0}
async def get_vector_stores(cache_duration=DEFAULT_VS_CACHE_DURATION):
    # (Keep implementation using cache and session fix)
    global vector_store_cache; now = time.time(); current_client = get_openai_client();
    if not current_client: return []
    if not vector_store_cache["list"] or now - vector_store_cache["last_updated"] > cache_duration:
        logger.info("Refreshing vector stores from OpenAI...")
        try:
            my_vector_stores = await asyncio.to_thread(current_client.vector_stores.list, order="desc", limit=50)
            vector_store_cache["list"] = [{"id": vs.id, "name": vs.name or f"Store ({vs.id[-6:]})"} for vs in my_vector_stores.data]
            vector_store_cache["last_updated"] = now; logger.info(f"Refreshed VS list: {len(vector_store_cache['list'])} VS.")
        except Exception as e: logger.error(f"Error fetching vector stores: {e}", exc_info=True); vector_store_cache["list"] = []
    else: logger.debug("Using cached vector store list.")
    return list(vector_store_cache["list"])

# --- Keep add_files_to_vector_store (Ensure it sets 'metadata') ---
async def add_files_to_vector_store(vector_store_id, file_paths_with_names, all_metadata_dict):
    """
    Uploads files individually, associates them with the vector store,
    and adds attributes provided by the user during upload.
    """
    current_client = get_openai_client()
    if not current_client: return {"status": "error", "message": "OpenAI client error."}
    if not file_paths_with_names: return {"status": "error", "message": "No files provided."}
    success_count = 0; failure_count = 0; total_count = len(file_paths_with_names); upload_results = []

    for temp_path, original_filename in file_paths_with_names:
        file_id = None; vs_file = None; file_stream = None
        try:
            logger.info(f"Processing file '{original_filename}' for VS {vector_store_id}...")

            # --- Get attributes for THIS file from the dictionary passed in ---
            # Use filename as the key used in frontend JS
            file_metadata = all_metadata_dict.get(original_filename, {})
            attributes = {
                "document_type": file_metadata.get("document_type", "general"), # Default if missing
                "language": file_metadata.get("language", "unknown"),       # Default if missing
                "category": file_metadata.get("category") or "",             # Handle optional field
                "original_filename": original_filename,
                "upload_unix_ts": int(time.time()),
                "processed_version": "1.1" # Indicate version using this upload method
            }
            # Remove empty category if needed
            if not attributes["category"]: del attributes["category"]
            logger.info(f"Using attributes for {file_id}: {attributes}")
            # --- End using provided attributes ---

            # --- Upload, Associate, Update (Keep this logic) ---
            file_stream = open(temp_path, "rb"); file_object = await asyncio.to_thread(current_client.files.create, file=file_stream, purpose="assistants")
            file_id = file_object.id; file_stream.close(); file_stream = None; logger.info(f"Uploaded '{original_filename}' as File ID: {file_id}")

            vs_file = await asyncio.to_thread(current_client.vector_stores.files.create_and_poll, vector_store_id=vector_store_id, file_id=file_id)
            vs_file_id = vs_file.id; logger.info(f"Associated File ID {file_id} ({original_filename}), Status: {vs_file.status}")

            if vs_file.status == 'completed':
                try: # Update attributes using the dictionary derived from user input
                    logger.info(f"Attempting update attributes for File ID {file_id}...")
                    await asyncio.to_thread(current_client.vector_stores.files.update, vector_store_id=vector_store_id, file_id=file_id, attributes=attributes)
                    logger.info(f"Attributes update call successful for File ID {file_id}.")
                    success_count += 1; upload_results.append(f"'{original_filename}': OK (Type: {attributes.get('document_type', '?')})")
                except Exception as update_err: logger.error(f"FAILED update attributes {file_id}: {update_err}", exc_info=True); failure_count += 1; upload_results.append(f"'{original_filename}': Added but ATTR UPDATE FAILED")
            else:
                logger.error(f"File {file_id} association failed. Status: {vs_file.status}")
                failure_count += 1
                upload_results.append(f"'{original_filename}': Failed assoc.")
                # Cleanup logic for failed association
                try:
                    logger.warning(f"Cleanup File {file_id}")
                    await asyncio.to_thread(current_client.files.delete, file_id=file_id)
                except Exception as del_err:
                    logger.error(f"Failed cleanup {file_id}: {del_err}")
        except Exception as e:
            logger.error(f"Error processing '{original_filename}': {e}", exc_info=True)
            failure_count += 1
            upload_results.append(f"'{original_filename}': Error")
        finally: # Cleanup
            if file_stream and not file_stream.closed:
                file_stream.close()
            try: os.remove(temp_path); logger.debug(f"Removed temp file: {temp_path}")
            except OSError as e_remove: logger.warning(f"Could not remove temp file {temp_path}: {e_remove}")

    final_message = f"Processed {total_count}. Success: {success_count}, Failed: {failure_count}."; status = "error" if failure_count == total_count else ("warning" if failure_count > 0 else "success")
    if failure_count > 0: final_message += " Details: " + " | ".join(upload_results[-failure_count:])
    return {"status": status, "message": final_message}


# --- Pydantic Models ---
class SourceMetadata(BaseModel):
    """Metadata about a source document used in retrieval."""
    file_id: str = Field(description="The ID of the file in the vector store")
    file_name: str = Field(description="The name of the file")
    section: Optional[str] = Field(None, description="Section or page information if available")
    confidence: float = Field(description="Confidence score for this source (0-1)")

class RetrievalSuccess(BaseModel):
    content: str
    source_filename: str
    sources: List[SourceMetadata] = Field(default_factory=list, description="Metadata about the sources of this content")

class RetrievalError(BaseModel):
    error_message: str
    details: Optional[str] = None
    query_attempted: Optional[str] = Field(None, description="The query that was attempted")

# Import the shared data model
from data.data_models import ExtractedData

class FinalAnswer(BaseModel):
    markdown_response: str
    sources_used: List[SourceMetadata] = Field(default_factory=list, description="Sources used to generate this response")

    model_config = ConfigDict(extra='ignore')

    def format_with_sources(self) -> str:
        """Format the response with source attribution."""
        if not self.sources_used:
            return self.markdown_response

        # Add sources section at the end
        sources_section = "\n\n## Sources\n"
        for i, source in enumerate(self.sources_used, 1):
            sources_section += f"{i}. **{source.file_name}** "
            if source.section:
                sources_section += f"(Section: {source.section}) "
            sources_section += f"- Confidence: {source.confidence:.2f}\n"

        return self.markdown_response + sources_section

    @classmethod
    def model_validate_json(cls, json_data, **kwargs):
        """Custom JSON validation to handle cases where markdown_response is an object"""
        try:
            # Try standard validation first
            return super().model_validate_json(json_data, **kwargs)
        except Exception as e:
            # If that fails, try to handle the case where markdown_response is an object
            try:
                data = json.loads(json_data)
                if isinstance(data.get('markdown_response'), dict):
                    # If markdown_response is a dict, convert it to a string
                    md_resp_dict = data['markdown_response']

                    # Case 1: It has title and content fields
                    title = md_resp_dict.get('title', '')
                    content = md_resp_dict.get('content', '')

                    # Case 2: It has title and type fields (common error pattern)
                    if 'type' in md_resp_dict and not content:
                        # This is the specific error case we're seeing
                        if title:
                            # Use the title as the content
                            data['markdown_response'] = f"# {title}\n\nNo detailed information available about this topic."
                        else:
                            # Generic fallback
                            data['markdown_response'] = "No information available about the knowledge base."
                    elif not content and title:
                        # If there's no content but there is a title, use the title
                        data['markdown_response'] = f"# {title}\n\nNo detailed information available about this topic."
                    elif title and content:
                        # If there's both title and content, format them
                        data['markdown_response'] = f"# {title}\n\n{content}"
                    else:
                        # Convert the entire dict to a string as a last resort
                        try:
                            data['markdown_response'] = json.dumps(md_resp_dict, indent=2)
                        except:
                            data['markdown_response'] = "No information available."

                    # Try validation again with the fixed data
                    return cls.model_validate(data)
                raise e  # Re-raise if we couldn't fix it
            except Exception as fix_error:
                # Log the error during fix attempt
                logger.error(f"Error during FinalAnswer validation fix: {fix_error}")
                # If our fix attempt failed, create a basic valid instance
                return cls(markdown_response="Sorry, there was an error processing the response. Please try again with a different query.")

class DOCXGenerationResult(BaseModel):
    status: str  # "success" or "error"
    file_path: Optional[str]  # Path to the generated file
    file_name: Optional[str]  # Filename for downloading
    message: str  # Success or error message

class AnalysisResult(BaseModel):
    intent: str = Field(default="kb_query", description="The determined intent of the user's query")
    details: Dict[str, Any] = Field(
        default_factory=dict,
        description="Additional details about the intent, such as query_topic, temp_file_names, required_fields, etc."
    )

# --- Tool Definitions ---

@function_tool(strict_mode=False)
async def list_knowledge_base_files(ctx: RunContextWrapper, vs_id: Optional[str] = None) -> Union[Dict[str, Any], Dict[str, str]]:
    """Lists all files in the knowledge base.

    This tool provides a list of all documents available in the specified knowledge base.
    Use this tool when the user asks about what's in the knowledge base or what documents are available.

    Args:
        vs_id: Optional vector store ID. If not provided, uses the one from context.

    Returns:
        A dictionary containing the list of files and their metadata.
    """
    logger.info(f"[Tool Call] list_knowledge_base_files")
    tool_context = ctx.context
    vector_store_id = vs_id or tool_context.get("vector_store_id")
    tool_client = tool_context.get("client")

    if not tool_client or not vector_store_id:
        return {"error": "Tool configuration error. Missing client or vector store ID."}

    try:
        # Get the list of files in the vector store
        files_response = await asyncio.to_thread(tool_client.vector_stores.files.list, vector_store_id=vector_store_id)

        if not files_response or not files_response.data:
            return {"files": [], "message": "No files found in the knowledge base."}

        # Process the files to get more details
        files_info = []
        for file_obj in files_response.data:
            try:
                # Get more details about the file
                file_details = await asyncio.to_thread(tool_client.files.retrieve, file_id=file_obj.id)

                # Extract relevant information
                file_info = {
                    "id": file_obj.id,
                    "filename": file_details.filename,
                    "purpose": getattr(file_details, "purpose", "unknown"),
                    "created_at": str(getattr(file_details, "created_at", "unknown")),
                    "size": getattr(file_details, "bytes", 0),
                }

                files_info.append(file_info)
            except Exception as file_err:
                logger.error(f"Error getting details for file {file_obj.id}: {file_err}")
                # Include basic info even if details retrieval fails
                files_info.append({"id": file_obj.id, "filename": getattr(file_obj, "filename", f"File-{file_obj.id[-6:]}")})

        return {
            "files": files_info,
            "count": len(files_info),
            "vector_store_id": vector_store_id,
            "message": f"Found {len(files_info)} files in the knowledge base."
        }
    except Exception as e:
        logger.error(f"Error listing knowledge base files: {e}", exc_info=True)
        return {"error": f"Failed to list knowledge base files: {str(e)}"}

@function_tool(strict_mode=False)
async def get_kb_document_content(ctx: RunContextWrapper, document_type: str, query_or_identifier: str, included_file_ids: Optional[List[str]] = Field(None, description="Optional list of file IDs to limit the search/retrieval to.")) -> Union[RetrievalSuccess, RetrievalError]:
    """Retrieves content from the knowledge base (Vector Store) based on document type and query/identifier."""
    logger.info(f"[Tool Call] get_kb_document_content: type='{document_type}', query='{query_or_identifier[:50]}...'")
    tool_context = ctx.context
    vs_id = tool_context.get("vector_store_id")
    tool_client = tool_context.get("client")
    chat_id = tool_context.get("chat_id")
    if not tool_client or not vs_id:
        return RetrievalError(error_message="Tool config error.")

    # Get file inclusion settings from parameter or context
    file_ids_to_filter = []

    # 1. Prioritize the 'included_file_ids' parameter if provided by the agent
    if included_file_ids is not None and len(included_file_ids) > 0:
        file_ids_to_filter = included_file_ids
        logger.info(f"Using file IDs from tool parameter: {file_ids_to_filter}")
    # 2. If parameter not provided, fall back to chat_db lookup (original logic)
    elif chat_id and chat_db:
        try:
            chat_files = await asyncio.to_thread(chat_db.get_chat_files, chat_id)
            file_ids_to_filter = [file["file_id"] for file in chat_files if file["included"]]
            if file_ids_to_filter:
                logger.info(f"Using {len(file_ids_to_filter)} included files for chat {chat_id} from DB")
        except Exception as e:
            logger.warning(f"Error getting included files for chat {chat_id} from DB: {e}")
    else:
        # No specific file IDs provided via parameter or chat context
        logger.info("No specific file IDs for filtering. Performing general search.")

    # Prepare filter information for cache lookup
    filter_info = {
        'document_type': document_type,
        'included_file_ids': file_ids_to_filter
    }

    # Check semantic cache first
    try:
        cached_result = await semantic_search_cache.find_semantic_match(
            new_query=query_or_identifier,
            vector_store_id=vs_id,
            filters=filter_info,
            chat_id=chat_id or "default"
        )

        if cached_result:
            logger.info(f"[SEMANTIC CACHE HIT] Retrieved semantically equivalent result for query: '{query_or_identifier[:30]}...'")
            # Ensure the cached result is the correct type
            if isinstance(cached_result, dict) and 'content' in cached_result and 'source_filename' in cached_result:
                return RetrievalSuccess(**cached_result)
            return cached_result
    except Exception as cache_err:
        logger.warning(f"Semantic cache lookup failed: {cache_err}. Falling back to direct search.")

    # We've already determined file_ids_to_filter above, so we'll use that instead of fetching again

    # First try with a filter for more precise results
    try:
        # Build filters
        filters = []

        # Add document type filter if specified and not a general type
        if document_type and document_type.lower() not in ["kb", "knowledge base", "general", "unknown", "none", ""]:
            logger.info(f"Adding document_type filter for: {document_type}")
            filters.append({"type": "eq", "key": "document_type", "value": document_type})
        else:
            logger.info(f"Not filtering by document_type: '{document_type}' is considered general")

        # Add file filter if we have files to filter
        if file_ids_to_filter:
            # OpenAI API doesn't support 'in' filter type, so we need to use 'or' with multiple 'eq' filters
            if len(file_ids_to_filter) == 1:
                # If only one file ID, use a simple 'eq' filter
                filters.append({"type": "eq", "key": "id", "value": file_ids_to_filter[0]})
            elif len(file_ids_to_filter) > 1:
                # If multiple file IDs, use 'or' with multiple 'eq' filters
                file_filters = []
                for file_id in file_ids_to_filter:
                    file_filters.append({"type": "eq", "key": "id", "value": file_id})

                # Add the combined OR filter
                if file_filters:
                    filters.append({"type": "or", "filters": file_filters})

        # Create filter object if we have any filters
        filter_obj = None
        if len(filters) > 1:
            # If multiple filters, combine with AND
            filter_obj = {"type": "and", "filters": filters}
        elif len(filters) == 1:
            # If single filter, use it directly
            filter_obj = filters[0]

        # Define search variants to run in parallel
        search_tasks = []
        search_variant_descriptions = []

        # Variant 1: Search with all filters (if we have any)
        if filter_obj:
            search_params = {
                "vector_store_id": vs_id,
                "query": query_or_identifier,
                "filters": filter_obj,
                "max_num_results": MAX_SEARCH_RESULTS_TOOL,
                "ranking_options": {"ranker": SEARCH_RANKER}
            }
            search_tasks.append(asyncio.to_thread(tool_client.vector_stores.search, **search_params))
            search_variant_descriptions.append(f"All filters: {json.dumps(filter_obj, indent=2)}")

        # Variant 2: Search with just file filters (if we have files to filter)
        if file_ids_to_filter:
            # Create file filter using supported filter types
            if len(file_ids_to_filter) == 1:
                file_filter = {"type": "eq", "key": "id", "value": file_ids_to_filter[0]}
            else:
                file_filters = []
                for file_id in file_ids_to_filter:
                    file_filters.append({"type": "eq", "key": "id", "value": file_id})
                file_filter = {"type": "or", "filters": file_filters}

            # Only add this variant if it's different from the first one
            if not filter_obj or filter_obj != file_filter:
                search_params = {
                    "vector_store_id": vs_id,
                    "query": query_or_identifier,
                    "filters": file_filter,
                    "max_num_results": MAX_SEARCH_RESULTS_TOOL,
                    "ranking_options": {"ranker": SEARCH_RANKER}
                }
                search_tasks.append(asyncio.to_thread(tool_client.vector_stores.search, **search_params))
                search_variant_descriptions.append(f"File-only filter: {json.dumps(file_filter, indent=2)}")

        # Variant 3: Search with just document type filter (if specified and not general)
        if document_type and document_type.lower() not in ["kb", "knowledge base", "general", "unknown", "none", ""]:
            doc_type_filter = {"type": "eq", "key": "document_type", "value": document_type}

            # Only add this variant if it's different from previous ones
            if (not filter_obj or filter_obj != doc_type_filter) and (not file_ids_to_filter or doc_type_filter != file_filter):
                search_params = {
                    "vector_store_id": vs_id,
                    "query": query_or_identifier,
                    "filters": doc_type_filter,
                    "max_num_results": MAX_SEARCH_RESULTS_TOOL,
                    "ranking_options": {"ranker": SEARCH_RANKER}
                }
                search_tasks.append(asyncio.to_thread(tool_client.vector_stores.search, **search_params))
                search_variant_descriptions.append(f"Document-type-only filter: {json.dumps(doc_type_filter, indent=2)}")

        # Variant 4: Search without any filters (semantic fallback)
        search_params = {
            "vector_store_id": vs_id,
            "query": query_or_identifier,
            "max_num_results": MAX_SEARCH_RESULTS_TOOL,
            "ranking_options": {"ranker": SEARCH_RANKER}
        }
        search_tasks.append(asyncio.to_thread(tool_client.vector_stores.search, **search_params))
        search_variant_descriptions.append("No filters (semantic fallback)")

        # Log the search variants
        logger.info(f"[CACHE MISS] Running {len(search_tasks)} parallel search variants for query: '{query_or_identifier[:50]}...'")
        for i, desc in enumerate(search_variant_descriptions):
            logger.info(f"Search variant {i+1}: {desc}")

        # Run all search variants in parallel
        search_results_list = await asyncio.gather(*search_tasks)

        # Process results from all variants
        combined_results = []
        result_counts = []

        for i, results in enumerate(search_results_list):
            if results and results.data:
                count = len(results.data)
                result_counts.append(count)
                logger.info(f"Search variant {i+1} returned {count} results")
                combined_results.extend(results.data)
            else:
                result_counts.append(0)
                logger.info(f"Search variant {i+1} returned no results")

        # If we have any results, use them
        if combined_results:
            # Deduplicate results based on file_id and content hash
            unique_results = {}
            for res in combined_results:
                # Create a unique key for each result based on file_id and content hash
                content_hash = hashlib.md5(''.join(part.text for part in res.content if part.type == 'text').encode()).hexdigest()
                key = f"{res.file_id}_{content_hash}"

                # Only keep the result with the highest score if we have duplicates
                if key not in unique_results or res.score > unique_results[key].score:
                    unique_results[key] = res

            # Convert back to list and sort by score
            deduplicated_results = list(unique_results.values())
            deduplicated_results.sort(key=lambda x: x.score, reverse=True)

            # Limit to MAX_SEARCH_RESULTS_TOOL
            final_results = deduplicated_results[:MAX_SEARCH_RESULTS_TOOL]

            logger.info(f"Combined {sum(result_counts)} results, deduplicated to {len(deduplicated_results)}, returning top {len(final_results)}")

            # Create a dummy search results object with our final results
            search_results = type('obj', (object,), {'data': final_results})
        else:
            # No results from any variant
            search_results = None

        if search_results and search_results.data:
            content = "\n\n".join(re.sub(r'\s+', ' ', part.text).strip() for res in search_results.data for part in res.content if part.type == 'text')
            source_filename = search_results.data[0].filename or f"FileID:{search_results.data[0].file_id[-6:]}"
            logger.info(f"[Tool Result] KB Content Found for query '{query_or_identifier[:30]}...'. Len: {len(content)}")

            # Collect source metadata
            sources = []
            for res in search_results.data:
                source = SourceMetadata(
                    file_id=res.file_id,
                    file_name=res.filename or f"FileID:{res.file_id[-6:]}",
                    section=None,  # Would need to extract from metadata if available
                    confidence=res.score  # Assuming score is available
                )
                sources.append(source)

            # Create result object with sources
            result = RetrievalSuccess(
                content=content,
                source_filename=source_filename,
                sources=sources
            )

            # Store in semantic cache
            try:
                await semantic_search_cache.set(
                    vector_store_id=vs_id,
                    query=query_or_identifier,
                    filters=filter_info,
                    chat_id=chat_id or "default",
                    document_type=document_type,
                    result=result
                )
                logger.info(f"[CACHE SET] Stored new result in semantic cache")
            except Exception as cache_err:
                logger.warning(f"Failed to store result in semantic cache: {cache_err}")

            return result
        else:
            logger.warning(f"[Tool Result] No KB content found for query: '{query_or_identifier[:50]}...'")
            error_result = RetrievalError(error_message=f"No KB content found for query related to '{document_type}'.")

            # Also cache negative results to avoid repeated failed searches
            try:
                await semantic_search_cache.set(
                    vector_store_id=vs_id,
                    query=query_or_identifier,
                    filters=filter_info,
                    chat_id=chat_id or "default",
                    document_type=document_type,
                    result=error_result
                )
                logger.info(f"[CACHE SET] Stored negative result in semantic cache")
            except Exception as cache_err:
                logger.warning(f"Failed to store negative result in semantic cache: {cache_err}")

            return error_result
    except Exception as e:
        logger.error(f"[Tool Error] KB Search failed for query '{query_or_identifier[:30]}...': {e}", exc_info=True)
        return RetrievalError(error_message=f"KB Search error: {str(e)}")

@function_tool(strict_mode=False)
async def process_temporary_file(ctx: RunContextWrapper, filename: str) -> Union[RetrievalSuccess, RetrievalError]:
    """Reads and returns the text content of a previously uploaded temporary file for use as context."""
    logger.info(f"[Tool Call] process_temporary_file: filename='{filename}'")
    tool_context = ctx.context
    temp_file_info = tool_context.get("temp_file_info")
    if not temp_file_info or temp_file_info.get("filename") != filename:
        return RetrievalError(error_message=f"Temporary file '{filename}' not available.")
    file_path = temp_file_info.get("path")
    if not file_path or not os.path.exists(file_path):
        return RetrievalError(error_message=f"Temporary file path invalid for '{filename}'.")
    try:
        text_content = ""
        file_lower = filename.lower()
        if file_lower.endswith(".pdf"):
            # Use our robust PDF text extraction function
            text_content = extract_text_from_pdf(file_path)
        elif file_lower.endswith((".txt", ".md")):
            with open(file_path, 'r', encoding='utf-8') as f:
                text_content = f.read()
        else:
            return RetrievalError(error_message=f"Unsupported temporary file type: {filename}")
        cleaned_content = re.sub(r'\s+', ' ', text_content).strip()
        logger.info(f"[Tool Result] Processed temporary file '{filename}'. Length: {len(cleaned_content)}")
        return RetrievalSuccess(content=cleaned_content, source_filename=f"Uploaded: {filename}")
    except Exception as e:
        logger.error(f"[Tool Error] Failed read/process temp file '{filename}': {e}", exc_info=True)
        return RetrievalError(error_message=f"Error processing temp file: {str(e)}")

@function_tool(strict_mode=False)
async def process_temp_file(ctx: RunContextWrapper, file_id: str = None, filename: str = None) -> Union[RetrievalSuccess, RetrievalError]:
    """Alias for process_temporary_file that handles both file_id and filename parameters.
    Reads and returns the text content of a previously uploaded temporary file for use as context."""
    logger.info(f"[Tool Call] process_temp_file: file_id='{file_id}', filename='{filename}'")

    # Use file_id as filename if provided, otherwise use filename
    actual_filename = file_id if file_id is not None else filename

    if actual_filename is None:
        return RetrievalError(error_message="Either file_id or filename must be provided.")

    # Call the original function with the determined filename
    return await process_temporary_file(ctx, actual_filename)

@function_tool(strict_mode=False)
async def retrieve_template_content(ctx: RunContextWrapper, template_name: str) -> Union[RetrievalSuccess, RetrievalError]:
    """Retrieves the text content of a specified document template (txt, md, pdf)."""
    logger.info(f"[Tool Call] retrieve_template_content: template_name='{template_name}'")
    try:
        # Use asyncio.to_thread to run the file operations asynchronously
        async def process_template():
            # Sanitize template_name but preserve extension
            original_filename = secure_filename(template_name)
            base_name, ext = os.path.splitext(original_filename)
            if not ext: ext = ".md" # Default to markdown if no extension provided

            # Check if extension is allowed for templates
            allowed_template_exts = {'.txt', '.md', '.pdf'}
            if ext.lower() not in allowed_template_exts:
                logger.error(f"Attempted to retrieve template with unsupported extension: {original_filename}")
                return RetrievalError(error_message=f"Unsupported template file type '{ext}'. Allowed: {', '.join(allowed_template_exts)}")

            final_filename = f"{base_name}{ext}" # Reconstruct potentially sanitized name
            template_path = os.path.join(TEMPLATE_DIR, final_filename)

            # Security check: Ensure path is still within TEMPLATE_DIR
            if not os.path.exists(template_path) or os.path.commonpath([TEMPLATE_DIR]) != os.path.commonpath([TEMPLATE_DIR, template_path]):
                logger.error(f"[Tool Error] Template file not found or invalid path: {template_path}")
                return RetrievalError(error_message=f"Template '{template_name}' not found.")

            # --- Extract content based on type ---
            content = ""
            logger.info(f"Reading template file: {template_path}")
            if ext.lower() == ".pdf":
                # Use our robust PDF text extraction function
                content = extract_text_from_pdf(template_path)
            elif ext.lower() in [".txt", ".md"]:
                with open(template_path, 'r', encoding='utf-8') as f:
                    content = f.read()

            if not content:
                logger.warning(f"Extracted empty content from template: {final_filename}")
                return RetrievalError(error_message=f"Could not extract content from template '{final_filename}'.")

            logger.info(f"[Tool Result] Retrieved template '{final_filename}'. Length: {len(content)}")
            cleaned_content = re.sub(r'\s+', ' ', content).strip()
            return RetrievalSuccess(content=cleaned_content, source_filename=f"Template: {original_filename}")

        # Run the template processing function asynchronously
        return await process_template()

    except Exception as e:
         logger.error(f"[Tool Error] Error retrieving template '{template_name}': {e}", exc_info=True)
         return RetrievalError(error_message=f"Error retrieving template: {str(e)}")

@function_tool(strict_mode=False)
async def generate_docx_from_markdown(ctx: RunContextWrapper, markdown_content: str, template_name: str) -> DOCXGenerationResult:
    """Converts markdown content into a professionally formatted DOCX file.

    Args:
        markdown_content: The populated markdown content from the template
        template_name: Name of the original template for reference

    Returns:
        Object containing the path to the generated DOCX file and status
    """
    try:
        # Import the docx_generator module
        import docx_generator

        # Generate the DOCX file asynchronously
        file_path, file_name = await asyncio.to_thread(docx_generator.markdown_to_docx, markdown_content, template_name)

        # Return success result
        return DOCXGenerationResult(
            status="success",
            file_path=file_path,
            file_name=file_name,
            message="DOCX file successfully generated"
        )
    except Exception as e:
        logger.error(f"Error generating DOCX: {e}", exc_info=True)
        return DOCXGenerationResult(
            status="error",
            file_path=None,
            file_name=None,
            message=f"Error generating DOCX: {str(e)}"
        )

@function_tool(strict_mode=False)
async def extract_data_for_template(ctx: RunContextWrapper, context_sources: List[str], required_fields: List[str], document_analyses: Optional[List[Dict]] = None) -> ExtractedData:
    """Extracts specific data fields required for a template from provided text context sources."""
    logger.info(f"[Tool Call] extract_data_for_template. Required: {required_fields}. Sources: {len(context_sources)} provided.")

    if document_analyses:
        logger.info(f"Document analyses provided for extraction: {len(document_analyses)}")

    # Call the integrated DocumentAnalyzerAgent implementation
    return await document_analyzer_integration.extract_data_for_template_integrated(ctx, context_sources, required_fields, document_analyses)

# --- Helper Functions for Workflow ---
async def detect_required_fields_from_template(template_content: str, template_name: str) -> List[str]:
    """Dynamically detect required fields from a template based on content analysis."""
    logger.info(f"Attempting to detect required fields from template: {template_name}")

    # Create a context wrapper with the OpenAI client
    ctx = RunContextWrapper({"client": get_openai_client()})

    # Create a combined input object with both template_content and template_name
    combined_input = {"template_content": template_content, "template_name": template_name}

    # Call the detect_fields_from_template tool directly using on_invoke_tool with only ctx and the combined input
    return await detect_fields_from_template.on_invoke_tool(ctx, combined_input)

def extract_final_answer(run_result: RunResult) -> str:
    """Extracts markdown response from FinalAnswer Pydantic model in RunResult,
       with robust fallbacks for plain text output or extraction errors."""
    try:
        logger.info(f"[DEBUG TRACE] extract_final_answer called with RunResult type: {type(run_result)}")
        final_output = run_result.final_output
        logger.info(f"[DEBUG TRACE] final_output type: {type(final_output)}")

        # Case 1: The intended and validated structured output
        if isinstance(final_output, FinalAnswer):
            logger.info(f"[DEBUG TRACE] Found FinalAnswer instance, calling format_with_sources()")
            result = final_output.format_with_sources()
            logger.info(f"[DEBUG TRACE] format_with_sources() returned: {result[:200]}...")
            return result

        # --- Start Robust Fallback Logic ---
        logger.warning(f"[DEBUG TRACE] Expected FinalAnswer, but final_output was type: {type(final_output)}. Attempting fallbacks.")

        # Fallback A: Check the messages list for the last assistant message
        # This is the most common place for an agent's text output
        if hasattr(run_result, 'messages') and isinstance(run_result.messages, list):
            logger.info(f"[DEBUG TRACE] Checking {len(run_result.messages)} messages for assistant content.")
            for msg in reversed(run_result.messages):
                 # Check for both dictionary format (from previous logs) and potential object format
                 if isinstance(msg, dict) and msg.get('role') == 'assistant' and msg.get('content'):
                       logger.info(f"[DEBUG TRACE] Fallback A: Found assistant message (dict).")
                       return msg.get('content')
                 if hasattr(msg, 'role') and hasattr(msg, 'content') and msg.role == 'assistant' and isinstance(msg.content, str):
                       logger.info(f"[DEBUG TRACE] Fallback A: Found assistant message (object).")
                       return msg.content

        # Fallback B: Check if final_output is a dictionary with markdown_response
        if isinstance(final_output, dict) and 'markdown_response' in final_output:
            logger.info(f"[DEBUG TRACE] Fallback B: Found dict with markdown_response key")
            md_resp = final_output['markdown_response']

            # If markdown_response is a string, use it directly
            if isinstance(md_resp, str):
                logger.info(f"[DEBUG TRACE] Fallback B: markdown_response is string, returning directly")
                return md_resp
            # If it's a dict with title/content, format it nicely
            elif isinstance(md_resp, dict):
                title = md_resp.get('title', '')
                content = md_resp.get('content', '')
                if title:
                    result = f"# {title}\n\n{content if content else 'No detailed information available about this topic.'}"
                    logger.info(f"[DEBUG TRACE] Fallback B: Formatted title/content")
                    return result

        # Fallback C: Check if final_output is a string
        if isinstance(final_output, str):
            logger.info(f"[DEBUG TRACE] Fallback C: final_output is string, returning directly")
            return final_output

        # Fallback D: Try to_input_list() to find the last assistant message
        # This is the original Case 5 fallback logic
        if hasattr(run_result, 'to_input_list'):
            try:
                logger.info(f"[DEBUG TRACE] Fallback D: Checking to_input_list() for assistant messages")
                history_list = run_result.to_input_list()
                # Find the last message in the history list generated by an assistant
                last_msg = next((msg for msg in reversed(history_list) if msg.get('role') == 'assistant'), None)
                if last_msg and isinstance(last_msg.get('content'), str):
                    logger.info(f"[DEBUG TRACE] Fallback D: Found assistant message in history")
                    return last_msg.get('content')
            except Exception as e:
                logger.error(f"[DEBUG TRACE] Fallback D: Error in to_input_list(): {e}")
                # Continue to next fallback

        # Fallback E: Check the raw_response for a potential text output
        if hasattr(run_result, 'raw_response') and run_result.raw_response:
            logger.warning(f"[DEBUG TRACE] Fallback E: Checking raw_response type: {type(run_result.raw_response)}")
            # Convert raw_response to string if necessary, and check if it seems like text
            raw_text = str(run_result.raw_response) # Safe conversion
            if raw_text and len(raw_text) > 10: # Basic check if it's likely actual content
                # Try to extract JSON if it looks like it contains JSON
                if '{' in raw_text and '}' in raw_text:
                    try:
                        start_idx = raw_text.find('{')
                        end_idx = raw_text.rfind('}')
                        if start_idx >= 0 and end_idx > start_idx:
                            json_str = raw_text[start_idx:end_idx+1]
                            data = json.loads(json_str)
                            if 'markdown_response' in data:
                                md_resp = data['markdown_response']
                                if isinstance(md_resp, str):
                                    logger.info(f"[DEBUG TRACE] Fallback E: Found markdown_response in JSON")
                                    return md_resp
                    except Exception as json_err:
                        logger.error(f"[DEBUG TRACE] Fallback E: JSON extraction failed: {json_err}")
                        # Continue with raw text

                logger.warning(f"[DEBUG TRACE] Fallback E: Returning raw_response as string.")
                return raw_text # Return the raw text

        # If all fallbacks fail
        logger.error("[DEBUG TRACE] All extraction fallbacks failed. No usable output found.")
        # Return a generic error message
        return "Sorry, I couldn't process the knowledge base information. Please try again."

    except Exception as e:
        # Catch any errors during extraction logic itself
        logger.error(f"[DEBUG TRACE] Critical Error during answer extraction logic: {e}", exc_info=True)
        # Return a specific error message about the extraction failure
        return f"Sorry, an internal error occurred while interpreting the AI's response: {str(e)}"

# --- Helper Functions for KB File Comparison ---
def extract_tool_output(run_result, tool_name_to_find):
    """Extract the output of a specific tool from a RunResult.

    Args:
        run_result: The RunResult from a Runner.run call
        tool_name_to_find: The name of the tool to find

    Returns:
        The structured output of the tool, or None if not found
    """
    tool_call_id = None
    tool_structured_output = None

    # First, find the ToolCallItem for the desired tool to get its call_id
    for item in run_result.new_items:
        if isinstance(item, agents_items.ToolCallItem):
            # Check if raw_item has name attribute and it matches our tool name
            if hasattr(item, 'raw_item') and hasattr(item.raw_item, 'name') and item.raw_item.name == tool_name_to_find:
                # Found the ToolCallItem for the correct tool
                # Get the call_id from the raw_item
                if hasattr(item.raw_item, 'call_id'):
                    tool_call_id = item.raw_item.call_id
                    logger.info(f"Found ToolCallItem for '{tool_name_to_find}' with call_id: {tool_call_id}")
                    break  # Found the initiating tool call

    if tool_call_id is None:
        logger.error(f"Could not find ToolCallItem for tool '{tool_name_to_find}' in RunResult.new_items.")
        return None

    # Second, find the corresponding ToolCallOutputItem using the call_id
    for item in run_result.new_items:
        if isinstance(item, agents_items.ToolCallOutputItem):
            # Check if this output item belongs to the tool call id we found
            if hasattr(item, 'raw_item') and isinstance(item.raw_item, dict) and item.raw_item.get('call_id') == tool_call_id:
                # Assuming the tool's return value is directly in the 'output' attribute of the ToolCallOutputItem
                if hasattr(item, 'output'):
                    logger.info(f"Found matching ToolCallOutputItem with output type: {type(item.output)}")
                    tool_structured_output = item.output
                    logger.info(f"Successfully extracted structured output for tool '{tool_name_to_find}' linked by call_id {tool_call_id}.")
                    break  # Found the tool output

    if tool_structured_output is None:
        logger.error(f"Could not find ToolCallOutputItem linked to call_id {tool_call_id} in RunResult.new_items.")
        # Try the final_output as a fallback
        logger.info("Falling back to final_output for tool result")
        tool_structured_output = run_result.final_output

    return tool_structured_output

def select_files_for_comparison(files_info: List[Dict], user_query: str) -> List[Dict]:
    """Select which files to compare based on the user query.

    Args:
        files_info: List of file dictionaries from the KB
        user_query: The user's query

    Returns:
        List of file dictionaries to compare
    """
    # If the user specified file names, use those
    query_lower = user_query.lower()
    mentioned_files = []

    for file_info in files_info:
        filename_lower = file_info["filename"].lower()
        if filename_lower in query_lower:
            mentioned_files.append(file_info)

    # If specific files were mentioned, use those
    if len(mentioned_files) >= 2:
        logger.info(f"Found {len(mentioned_files)} files mentioned in the query")
        return mentioned_files

    # Otherwise, use the two most recently created files
    # Sort by created_at in descending order (most recent first)
    sorted_files = sorted(files_info, key=lambda x: x.get("created_at", ""), reverse=True)
    selected_files = sorted_files[:2] if len(sorted_files) >= 2 else sorted_files

    logger.info(f"Selected {len(selected_files)} most recent files for comparison")
    return selected_files

def create_comparison_prompt(file_contents: List[Dict], user_query: str) -> str:
    """Create a prompt for comparing file contents.

    Args:
        file_contents: List of dictionaries with file_info and content
        user_query: The user's query

    Returns:
        A prompt string for the comparison agent
    """
    prompt = f"""Compare the following files based on the user's query: "{user_query}"

"""

    # Add each file's content to the prompt
    for i, file_content in enumerate(file_contents):
        file_info = file_content["file_info"]
        content = file_content["content"]

        # Limit content length to avoid token limits
        max_content_length = 10000
        if len(content) > max_content_length:
            content = content[:max_content_length] + "... [content truncated]"

        prompt += f"""File {i+1}: {file_info['filename']}
Created: {file_info.get('created_at', 'Unknown')}
Size: {file_info.get('size', 'Unknown')} bytes
Content:
{content}

"""

    prompt += """YOUR TASK:
1. Compare these files and identify if they are identical, similar, or different.
2. If they are different, highlight the key differences.
3. Provide a detailed analysis based on the user's specific question.
4. Format your response in clear markdown with appropriate sections.

IMPORTANT: Your response MUST be a simple markdown string, not a complex object or JSON."""

    return prompt

async def run_kb_file_comparison(user_query: str, history: List[Dict[str, str]], workflow_context: Dict, vs_id: Optional[str] = None) -> str:
    """Handles file comparison queries for KB files.

    Args:
        user_query: The user's query
        history: The conversation history
        workflow_context: The workflow context containing any necessary information
        vs_id: Optional vector store ID (can be extracted from workflow_context if needed)
    """
    logger.info(f"Running KB File Comparison workflow for query: '{user_query[:50]}...'")

    try:
        # 1. Get the list of files to compare
        kb_context = workflow_context.copy()
        vector_store_id = vs_id or kb_context.get("vector_store_id")

        if not vector_store_id:
            return "Error: No vector store ID provided for KB file comparison."

        # Run the KB Query Agent to list the files
        agent_input = f"List the files available in the knowledge base. The user wants to compare files."

        kb_list_raw = await Runner.run(
            kb_query_agent,
            input=agent_input,
            context=kb_context
        )

        # Extract the file list using the helper function
        tool_structured_output = extract_tool_output(kb_list_raw, "list_knowledge_base_files")

        if not isinstance(tool_structured_output, dict) or "files" not in tool_structured_output:
            return "I couldn't retrieve the list of files to compare. Please try again."

        # Extract file information
        files_info = tool_structured_output.get("files", [])
        file_count = len(files_info)

        if file_count < 2:
            return "I need at least two files in the knowledge base to perform a comparison. Currently, there are not enough files available."

        # 2. Determine which files to compare based on the query
        files_to_compare = select_files_for_comparison(files_info, user_query)

        if len(files_to_compare) < 2:
            return "I need at least two files to perform a comparison. Please specify which files you'd like to compare or upload more files to the knowledge base."

        # 3. Retrieve the content of each file
        file_contents = []
        for file_info in files_to_compare:
            # Set up context with the file ID
            file_context = kb_context.copy()
            file_context["included_file_ids"] = [file_info["id"]]

            # Log the file context for debugging
            logger.info(f"[DEBUG CONTENT] file_context for {file_info['filename']}: {file_context}")
            logger.info(f"[DEBUG CONTENT] file_context keys: {list(file_context.keys())}")
            logger.info(f"[DEBUG CONTENT] included_file_ids: {file_context.get('included_file_ids')}")

            # --- Construct Input as a LIST of Messages ---
            # Embed the structured data within the message content as a string
            message_content = f"""
            Retrieve the full content from the knowledge base for a specific file for comparison.

            FILE DETAILS:
            - File ID: {file_info['id']}
            - Filename: {file_info['filename']}

            YOUR TASK:
            Use the 'get_kb_document_content' tool to retrieve the full text content for the file specified above.
            Pass the 'File ID' as the 'included_file_ids' parameter to the tool.
            Provide a generic query like "full content" or "retrieve" as the 'query_or_identifier' parameter.
            Provide 'general' as the 'document_type' parameter.
            Return the result of the tool call.
            """

            content_agent_input_messages = [
                {"role": "user", "content": message_content}
                # You could also include relevant history here if needed by the agent for context
                # *history
            ]
            # ---------------------------------------------

            logger.info(f"Attempting content retrieval for file: {file_info['filename']}")
            retrieved_success_object = None # Variable to hold the successful RetrievalSuccess object

            # --- Inner Tool Retry Loop ---
            max_tool_retries = 2
            tool_retry_delay = 1

            for tool_attempt in range(max_tool_retries):
                try:
                    logger.info(f"  Tool Attempt {tool_attempt + 1} of {max_tool_retries}: Calling agent to get content for {file_info['filename']}")
                    content_raw = await Runner.run(
                        kb_query_agent,
                        input=content_agent_input_messages,
                        context=kb_context
                    )

                    # --- Extract Tool Output Object from new_items ---
                    retrieval_tool_output = None
                    tool_name_to_find = "get_kb_document_content"

                    if hasattr(content_raw, 'new_items') and isinstance(content_raw.new_items, list):
                        # Find the ToolCallItem to get the call_id
                        tool_call_id = None
                        for item in content_raw.new_items:
                            if isinstance(item, agents_items.ToolCallItem) and hasattr(item, 'raw_item') and hasattr(item.raw_item, 'name') and item.raw_item.name == tool_name_to_find:
                                 if hasattr(item.raw_item, 'call_id'):
                                      tool_call_id = item.raw_item.call_id
                                      break

                        # If ToolCallItem was found, find the corresponding ToolCallOutputItem
                        if tool_call_id:
                            for item in content_raw.new_items:
                                 if isinstance(item, agents_items.ToolCallOutputItem):
                                     if hasattr(item, 'raw_item') and isinstance(item.raw_item, dict) and item.raw_item.get('call_id') == tool_call_id:
                                          # This 'output' attribute should hold the actual object returned by the tool function
                                          if hasattr(item, 'output'):
                                               retrieval_tool_output = item.output
                                               logger.info(f"  Tool Attempt {tool_attempt + 1}: Successfully extracted tool output object from new_items for {tool_name_to_find}.")
                                               break
                    # --- End Extraction ---

                    # Check if we got a valid RetrievalSuccess object
                    if isinstance(retrieval_tool_output, RetrievalSuccess) and retrieval_tool_output.content: # Also check if content is not empty
                        logger.info(f"  Tool Attempt {tool_attempt + 1}: Successfully retrieved and validated content for file {file_info['filename']}.")
                        retrieved_success_object = retrieval_tool_output # Store the successful object
                        break # *** Exit the INNER loop immediately on success ***

                    elif isinstance(retrieval_tool_output, RetrievalError):
                         logger.warning(f"  Tool Attempt {tool_attempt + 1}: Retrieval tool returned RetrievalError: {retrieval_tool_output.error_message}. Retrying...")

                    else: # Unexpected output type from tool
                         logger.warning(f"  Tool Attempt {tool_attempt + 1}: Retrieval tool output was unexpected type ({type(retrieval_tool_output)}). Retrying...")

                except Exception as e:
                    logger.error(f"  Tool Attempt {tool_attempt + 1}: Error during agent/tool run for content retrieval for {file_info['filename']}: {e}", exc_info=True)
                    # The retrieved_success_object will remain None

                if retrieved_success_object is None and tool_attempt < max_tool_retries - 1:
                     logger.info(f"  Tool Attempt {tool_attempt + 1} failed. Waiting {tool_retry_delay}s before retrying content retrieval for {file_info['filename']}...")
                     await asyncio.sleep(tool_retry_delay)
                     # Optional: Exponential backoff

            # --- After the Inner Tool Retry Loop ---

            # Check if we successfully got the object after retries
            if retrieved_success_object is not None:
                # *** Add the file content to file_contents ONLY here if successful ***
                file_contents.append({
                    "file_info": file_info,
                    "content": retrieved_success_object.content # Access content from the stored object
                })
                logger.info(f"Added content for file {file_info['filename']} to file_contents list.") # Log success clearly
            else:
                 # If retrieved_success_object is still None after all retries
                 logger.warning(f"Content retrieval for file {file_info['filename']} failed after {max_tool_retries} attempts. Skipping this file for comparison.")
                 # The main check after the loop 'if len(file_contents) < 2:' will handle if not enough files were successful.

        if len(file_contents) < 2:
            return "I couldn't retrieve enough file content to perform a comparison. Please try again or specify different files."

        # 4. Create a comparison prompt for the analysis agent
        comparison_prompt = create_comparison_prompt(file_contents, user_query)

        # Create messages for the synthesizer
        synthesis_messages = history + [{
            "role": "user",
            "content": comparison_prompt
        }]

        # Initialize with a fallback message
        final_markdown_response = "Sorry, something went wrong while comparing the files."

        # 5. Run the comparison analysis with retries
        max_retries = 3  # Number of times to retry the synthesis call
        retry_delay_seconds = 2  # Initial delay before retrying
        synthesizer_success = False  # Flag to track if synthesis succeeded

        for attempt in range(max_retries):
            try:
                logger.info(f"Attempt {attempt + 1} of {max_retries}: Calling meta_query_synthesizer_agent to compare files...")

                # Run the meta-query synthesizer agent with the comparison prompt
                final_synth_raw = await Runner.run(
                    meta_query_synthesizer_agent,
                    input=synthesis_messages,
                    context=workflow_context
                )

                # Check for success
                synthesizer_success = False  # Assume failure unless proven otherwise

                # First, check if final_output is a non-empty string
                if final_synth_raw and hasattr(final_synth_raw, 'final_output') and isinstance(final_synth_raw.final_output, str) and final_synth_raw.final_output.strip():
                    logger.info(f"Attempt {attempt + 1}: Synthesizer produced string content in final_output.")
                    logger.info(f"Final output string preview: {final_synth_raw.final_output[:200]}...")
                    synthesizer_success = True  # Set success flag
                # If final_output was NOT a string or was empty, check messages as fallback
                elif final_synth_raw and hasattr(final_synth_raw, 'messages'):
                    logger.info(f"Attempt {attempt + 1}: final_output was not string, checking messages for assistant content.")
                    assistant_message_content = None
                    for msg in reversed(final_synth_raw.messages):  # Check recent messages first
                        if hasattr(msg, 'role') and hasattr(msg, 'content'):
                            if msg.role == 'assistant' and msg.content:
                                assistant_message_content = msg.content
                                break  # Found the assistant's response
                        elif isinstance(msg, dict) and msg.get('role') == 'assistant' and msg.get('content'):
                            assistant_message_content = msg.get('content')
                            break  # Found the assistant's response

                    if assistant_message_content:
                        logger.info(f"Attempt {attempt + 1}: Fallback success check: Found assistant message.")
                        logger.info(f"Assistant message content: {assistant_message_content[:200]}...")
                        synthesizer_success = True  # Set success flag

                # Exit the retry loop if we found a usable result
                if synthesizer_success:
                    logger.info(f"Attempt {attempt + 1}: Synthesis attempt determined successful.")
                    break
                else:
                    logger.warning(f"Attempt {attempt + 1}: Synthesizer run completed, but no usable output found in final_output or messages. Retrying...")

            except Exception as e:
                # Catch exceptions during the API call itself (timeouts, 502s, etc.)
                logger.error(f"Attempt {attempt + 1}: Error during synthesis API call: {e}", exc_info=True)
                synthesizer_success = False  # Ensure flag is false

            if not synthesizer_success and attempt < max_retries - 1:
                logger.info(f"Attempt {attempt + 1} failed. Waiting {retry_delay_seconds}s before retrying...")
                await asyncio.sleep(retry_delay_seconds)  # Wait before the next attempt
                retry_delay_seconds *= 2  # Exponential backoff

        # Final error handling after retries
        if not synthesizer_success:
            logger.error(f"All {max_retries} synthesis attempts failed.")
            return "Sorry, I repeatedly had trouble comparing the files due to an AI service issue. Please try again later."

        # 6. Extract and return the final answer
        final_markdown_response = extract_final_answer(final_synth_raw)

        # Add sources to the workflow context
        sources = []
        for file_content in file_contents:
            file_info = file_content["file_info"]
            source = SourceMetadata(
                file_id=file_info["id"],
                file_name=file_info["filename"],
                section=None,
                confidence=1.0
            )
            sources.append(source)

        workflow_context["kb_sources"] = sources

        return final_markdown_response

    except Exception as e:
        logger.error(f"KB file comparison workflow failed: {e}", exc_info=True)
        return f"Sorry, an error occurred while comparing the files: {html.escape(str(e))}"

async def run_kb_workflow(user_query: str, history: List[Dict[str, str]], workflow_context: Dict, vs_id: Optional[str] = None) -> str:
    """Orchestrates the KB workflow based on the query type (meta vs. search).
    This function branches the workflow based on the kb_query_type determined by the analyzer.

    Args:
        user_query: The user's query
        history: The conversation history
        workflow_context: The workflow context containing any necessary information
        vs_id: Optional vector store ID (can be extracted from workflow_context if needed)
    """
    logger.info(f"Running KB workflow for query: '{user_query[:50]}...'")

    # Get the KB query type from the workflow context
    kb_query_type = workflow_context.get("kb_query_type")
    logger.info(f"[CRITICAL DEBUG] kb_query_type from workflow_context: '{kb_query_type}' (type: {type(kb_query_type)})")
    logger.info(f"[CRITICAL DEBUG] workflow_context keys: {list(workflow_context.keys())}")

    is_meta_query = kb_query_type == "meta"
    is_file_analysis = kb_query_type == "file_analysis"

    logger.info(f"[CRITICAL DEBUG] is_meta_query: {is_meta_query}")
    logger.info(f"[CRITICAL DEBUG] is_file_analysis: {is_file_analysis}")

    # Default error response
    final_markdown_response = "Error: KB workflow failed."

    try:
        if is_meta_query:
            # META-QUERY PATH: List KB files
            logger.info("Executing KB Meta-Query Workflow (List Files)...")
            final_markdown_response = await run_kb_meta_query(user_query, history, workflow_context, vs_id)
        elif is_file_analysis:
            # FILE ANALYSIS PATH: Analyze previously listed KB files
            logger.info("Executing KB File Analysis Workflow...")

            # Get the analysis type from the details if available
            analysis_type = workflow_context.get("analysis_type", "general_analysis")
            logger.info(f"File analysis type: {analysis_type}")

            # Add the analysis type to the workflow context
            workflow_context["analysis_type"] = analysis_type

            # --- START CRITICAL DEBUG LOG ---
            logger.info(f"[CRITICAL DEBUG] Checking analysis_type for comparison routing.")
            logger.info(f"[CRITICAL DEBUG]   analysis_type value: '{analysis_type}' (type: {type(analysis_type)})")
            logger.info(f"[CRITICAL DEBUG]   user_query.lower() value: '{user_query.lower()}'")
            logger.info(f"[CRITICAL DEBUG]   String literal comparison result (analysis_type == 'comparison'): {analysis_type == 'comparison'}")
            logger.info(f"[CRITICAL DEBUG]   'identical' in user_query.lower(): {'identical' in user_query.lower()}")
            logger.info(f"[CRITICAL DEBUG]   'same' in user_query.lower(): {'same' in user_query.lower()}")
            logger.info(f"[CRITICAL DEBUG]   'similar' in user_query.lower(): {'similar' in user_query.lower()}")
            logger.info(f"[CRITICAL DEBUG]   'compare' in user_query.lower(): {'compare' in user_query.lower()}")
            # --- END CRITICAL DEBUG LOG ---

            # Call the dedicated workflow function for comparison queries
            # Fix the condition by properly parenthesizing each part of the OR expression
            is_comparison_type = (analysis_type == "comparison")
            has_identical = ("identical" in user_query.lower())
            has_same = ("same" in user_query.lower())
            has_similar = ("similar" in user_query.lower())
            has_compare = ("compare" in user_query.lower())

            condition_result = (is_comparison_type or has_identical or has_same or has_similar or has_compare)

            logger.info(f"[CRITICAL DEBUG] Fixed condition components:")
            logger.info(f"[CRITICAL DEBUG]   is_comparison_type: {is_comparison_type}")
            logger.info(f"[CRITICAL DEBUG]   has_identical: {has_identical}")
            logger.info(f"[CRITICAL DEBUG]   has_same: {has_same}")
            logger.info(f"[CRITICAL DEBUG]   has_similar: {has_similar}")
            logger.info(f"[CRITICAL DEBUG]   has_compare: {has_compare}")
            logger.info(f"[CRITICAL DEBUG] Full fixed condition result: {condition_result}")

            if condition_result:
                # --- Add a log INSIDE this block ---
                logger.info("[CRITICAL DEBUG] *** Entering comparison workflow branch. ***")
                # --- Call the NEW comparison workflow ---
                final_markdown_response = await run_kb_file_comparison(user_query, history, workflow_context, vs_id)
                # -----------------------------------------
            else:
                # --- Add a log INSIDE this block ---
                logger.warning("[CRITICAL DEBUG] *** Falling through to non-comparison branch. ***")
                # For other (currently unimplemented) analysis types, keep old behavior or return message
                logger.warning(f"Analysis type '{analysis_type}' is not fully implemented yet. Listing files instead.")
                final_markdown_response = await run_kb_meta_query(user_query, history, workflow_context, vs_id)
                final_markdown_response += f"\n\n**Note:** Analysis type '{analysis_type}' is not yet fully supported."
        else:
            # SEARCH PATH: Standard RAG search
            logger.info("Executing KB Search Query Workflow...")
            final_markdown_response = await run_kb_search_query(user_query, history, workflow_context, vs_id)
    except Exception as e:
        logger.error(f"KB workflow failed: {e}", exc_info=True)
        final_markdown_response = f"Sorry, an error occurred during processing: {html.escape(str(e))}"

    return final_markdown_response

async def run_kb_meta_query(user_query: str, history: List[Dict[str, str]], workflow_context: Dict, vs_id: Optional[str] = None) -> str:
    """Handles meta-queries about the KB itself (listing files, etc.).

    Args:
        user_query: The user's query
        history: The conversation history
        workflow_context: The workflow context containing any necessary information
        vs_id: Optional vector store ID (can be extracted from workflow_context if needed)
    """
    logger.info(f"Running KB Meta-Query workflow for query: '{user_query[:50]}...'")
    try:
        # Prepare the input for the KB Query Agent
        kb_context = workflow_context.copy()
        vector_store_id = vs_id or kb_context.get("vector_store_id")

        if not vector_store_id:
            return "Error: No vector store ID provided for KB meta-query."

        # Create the agent input for listing files
        agent_input = f"List the files available in the knowledge base. The user wants to know what's in the KB."

        # Run the KB Query Agent to list the files
        kb_list_raw = await Runner.run(
            kb_query_agent,
            input=agent_input,
            context=kb_context
        )

        # Add basic logging for debugging
        logger.info(f"Type of kb_list_raw: {type(kb_list_raw)}")
        logger.info(f"Length of kb_list_raw.new_items: {len(kb_list_raw.new_items)}")

        # Extract the file list using the helper function
        tool_structured_output = extract_tool_output(kb_list_raw, "list_knowledge_base_files")

        if tool_structured_output is None:
            final_markdown_response = "Sorry, I encountered an internal issue processing the list of files."
            return final_markdown_response  # Exit

        # Process the result from the list tool
        if isinstance(tool_structured_output, dict) and "files" in tool_structured_output:
            # Extract file information
            files_info = tool_structured_output.get("files", [])
            file_count = tool_structured_output.get("count", len(files_info))

            # Import json for structured data formatting
            import json

            # Create structured file list as JSON for the synthesizer
            structured_file_list = json.dumps(files_info, indent=2)
            logger.info(f"Structured file list prepared with {file_count} files")

            # Create source metadata for the KB itself
            kb_source = SourceMetadata(
                file_id="kb_meta",
                file_name="Knowledge Base Index",
                section=None,
                confidence=1.0
            )
            workflow_context["kb_sources"] = [kb_source]

            # Create a prompt for the synthesizer agent with structured data
            is_file_analysis = workflow_context.get("kb_query_type") == "file_analysis"
            analysis_type = workflow_context.get("analysis_type", "general_analysis") if is_file_analysis else None

            if is_file_analysis:
                # Create a prompt for file analysis
                meta_prompt = f"""The user asked about the files in the knowledge base. This appears to be a follow-up question about previously listed files.

User query: "{user_query}"

Analysis type: {analysis_type}

Here is the structured data about the files in the knowledge base:

Knowledge Base File List Data (JSON):
{structured_file_list}

Total files: {file_count}

YOUR TASK:
1. Generate a clear, formatted list of these files with their details.
2. Include the filename, creation date, and size information in a readable format.
3. Then provide a specific analysis of these files based on the user's query.

Based on the analysis type "{analysis_type}", focus on:
- For "comparison": Compare the files in terms of size, date, and type
- For "size_analysis": Highlight size information and sort files by size
- For "date_analysis": Highlight date information and sort files by date
- For "content_analysis": Mention what types of content might be in these files based on their names and extensions
- For "general_analysis": Provide a general overview of the files

FORMATTING REQUIREMENTS:
- Use markdown formatting (headers, bullet points, etc.)
- Start with a clear header like "# Knowledge Base Files Analysis"
- List each file with its details in a consistent format
- Group files by type if appropriate
- Include a specific section addressing the user's query about these files

EXAMPLE OUTPUT FORMAT:
```
# Knowledge Base Files Analysis

The knowledge base contains {file_count} files:

## Documents
1. **document1.pdf** - Created: 2023-01-15, Size: 1.2 MB
2. **document2.docx** - Created: 2023-02-20, Size: 850 KB

## Analysis of Files
[Provide specific analysis based on the user's query and analysis type]
```

IMPORTANT: Your response MUST be a simple markdown string, not a complex object or JSON. Do not include any metadata or structure beyond the markdown content itself."""
            else:
                # Standard meta-query prompt
                meta_prompt = f"""The user asked about the contents of the knowledge base.

Here is the structured data about the files in the knowledge base:

Knowledge Base File List Data (JSON):
{structured_file_list}

Total files: {file_count}

YOUR TASK:
1. Generate a clear, formatted list of these files with their details.
2. Include the filename, creation date, and size information in a readable format.
3. Then provide a helpful explanation of what the knowledge base contains and how the user can use this information.

FORMATTING REQUIREMENTS:
- Use markdown formatting (headers, bullet points, etc.)
- Start with a clear header like "# Knowledge Base Contents"
- List each file with its details in a consistent format
- Group files by type if appropriate
- End with a brief explanation of how to use the knowledge base

EXAMPLE OUTPUT FORMAT:
```
# Knowledge Base Contents

The knowledge base contains {file_count} files:

## Documents
1. **document1.pdf** - Created: 2023-01-15, Size: 1.2 MB
2. **document2.docx** - Created: 2023-02-20, Size: 850 KB

## How to Use the Knowledge Base
You can ask questions about any of these documents...
```

IMPORTANT: Your response MUST be a simple markdown string, not a complex object or JSON. Do not include any metadata or structure beyond the markdown content itself."""

            synthesis_messages = history + [{
                "role": "user",
                "content": meta_prompt
            }]

            # Initialize with a fallback message
            final_markdown_response = "Sorry, something went wrong while generating the knowledge base file list."

            try:
                # Log the model being used
                model, _ = get_model_with_fallback()
                logger.info(f"Using model: {model} for meta-query synthesis")

                # --- START DEBUG TRACE LOGS (BEFORE SYNTHESIZER CALL) ---
                logger.info(f"[DEBUG TRACE] Input messages for meta_query_synthesizer_agent:")
                if synthesis_messages:
                    for i, msg in enumerate(synthesis_messages):
                         # Log the content of each message being sent to the synthesizer
                         logger.info(f"[DEBUG TRACE]   Synth Input Msg {i} Role: {msg.get('role', 'N/A')} Content: {msg.get('content', 'N/A')}")
                else:
                    logger.warning("[DEBUG TRACE]   Synthesis input messages list is empty.")
                logger.info("[DEBUG TRACE] Calling meta_query_synthesizer_agent to format the file list...")
                # --- END DEBUG TRACE LOGS (BEFORE SYNTHESIZER CALL) ---

                # --- START SYNTHESIS WITH RETRIES ---
                max_retries = 3  # Number of times to retry the synthesis call
                retry_delay_seconds = 2  # Initial delay before retrying
                synthesizer_success = False  # Flag to track if synthesis succeeded

                for attempt in range(max_retries):
                    try:
                        logger.info(f"Attempt {attempt + 1} of {max_retries}: Calling meta_query_synthesizer_agent to format the file list...")

                        # Run the specialized meta-query synthesizer agent with the structured file list
                        # This agent is already configured with output_type=str
                        final_synth_raw = await Runner.run(
                            meta_query_synthesizer_agent,
                            input=synthesis_messages,
                            context=workflow_context
                            # No need to override output_type as it's set in the agent definition
                        )

                        # --- Start Modified Success Check ---
                        synthesizer_success = False  # Assume failure unless proven otherwise

                        # First, check if final_output is a non-empty string (for meta_query_synthesizer_agent with output_type=str)
                        if final_synth_raw and hasattr(final_synth_raw, 'final_output') and isinstance(final_synth_raw.final_output, str) and final_synth_raw.final_output.strip():
                            logger.info(f"Attempt {attempt + 1}: Synthesizer produced string content in final_output.")
                            logger.info(f"Final output string preview: {final_synth_raw.final_output[:200]}...")
                            synthesizer_success = True  # Set success flag
                        # If final_output was NOT a string or was empty, check messages (original logic as fallback)
                        elif final_synth_raw and hasattr(final_synth_raw, 'messages'):
                            logger.info(f"Attempt {attempt + 1}: final_output was not string, checking messages for assistant content.")
                            assistant_message_content = None
                            for msg in reversed(final_synth_raw.messages):  # Check recent messages first
                                if hasattr(msg, 'role') and hasattr(msg, 'content'):
                                    if msg.role == 'assistant' and msg.content:
                                        assistant_message_content = msg.content
                                        break  # Found the assistant's response
                                elif isinstance(msg, dict) and msg.get('role') == 'assistant' and msg.get('content'):
                                    assistant_message_content = msg.get('content')
                                    break  # Found the assistant's response

                            if assistant_message_content:
                                logger.info(f"Attempt {attempt + 1}: Fallback success check: Found assistant message.")
                                logger.info(f"Assistant message content: {assistant_message_content[:200]}...")
                                synthesizer_success = True  # Set success flag

                        # Exit the retry loop if we found a usable result
                        if synthesizer_success:
                            logger.info(f"Attempt {attempt + 1}: Synthesis attempt determined successful.")
                            break
                        else:
                            logger.warning(f"Attempt {attempt + 1}: Synthesizer run completed, but no usable output found in final_output or messages. Retrying...")
                        # --- End Modified Success Check ---

                    except Exception as e:
                        # Catch exceptions during the API call itself (timeouts, 502s, etc.)
                        logger.error(f"Attempt {attempt + 1}: Error during synthesis API call: {e}", exc_info=True)
                        # Do NOT set final_markdown_response to the fallback here. Let the loop continue.
                        synthesizer_success = False  # Ensure flag is false

                    if not synthesizer_success and attempt < max_retries - 1:
                        logger.info(f"Attempt {attempt + 1} failed. Waiting {retry_delay_seconds}s before retrying...")
                        await asyncio.sleep(retry_delay_seconds)  # Wait before the next attempt
                        retry_delay_seconds *= 2  # Exponential backoff

                # --- END SYNTHESIS WITH RETRIES ---

                # --- Final Error Handling after Retries ---
                if not synthesizer_success:
                    logger.error(f"All {max_retries} synthesis attempts failed.")
                    # Set the user-friendly error message ONLY if all retries failed
                    final_markdown_response = "Sorry, I repeatedly had trouble generating the list of knowledge base files due to an AI service issue. Please try again later."
                    return final_markdown_response

                # If we reached here, we have a successful synthesis result
                if synthesizer_success:
                    # --- START DEBUG TRACE LOGS (AFTER SYNTHESIZER CALL) ---
                    logger.info(f"[DEBUG TRACE] Synthesizer Run completed successfully after {attempt+1} attempt(s).")
                    logger.info(f"[DEBUG TRACE]   RunResult type: {type(final_synth_raw)}")
                    logger.info(f"[DEBUG TRACE]   RunResult final_output type: {type(final_synth_raw.final_output) if final_synth_raw and hasattr(final_synth_raw, 'final_output') else 'N/A' }")
                    # Log the actual value of final_output
                    logger.info(f"[DEBUG TRACE]   RunResult final_output value: {final_synth_raw.final_output if final_synth_raw and hasattr(final_synth_raw, 'final_output') else 'N/A'}")

                    # Log the content of *all* messages in the result - this is the AI's conversation turn
                    if final_synth_raw and hasattr(final_synth_raw, 'messages') and final_synth_raw.messages:
                        logger.info(f"[DEBUG TRACE]   Synthesizer RunResult messages count: {len(final_synth_raw.messages)}")
                        for i, msg in enumerate(final_synth_raw.messages):
                             # Log the content of each message returned by the synthesizer
                             if hasattr(msg, 'role') and hasattr(msg, 'content'):
                                 logger.info(f"[DEBUG TRACE]     Synth Output Msg {i} Role: {msg.role} Content: {msg.content}")
                             elif isinstance(msg, dict):
                                 logger.info(f"[DEBUG TRACE]     Synth Output Msg {i} Role: {msg.get('role', 'N/A')} Content: {msg.get('content', 'N/A')}")
                             else:
                                 logger.info(f"[DEBUG TRACE]     Synth Output Msg {i} Type: {type(msg)}, Value: {msg}")
                    else:
                        logger.warning("[DEBUG TRACE]   Synthesizer RunResult.messages is empty or missing. AI might not have produced a message.")
                    # --- END DEBUG TRACE LOGS (AFTER SYNTHESIZER CALL) ---

                    # Log the last message from the synthesizer, as extract_final_answer likely uses this
                    if hasattr(final_synth_raw, 'messages') and final_synth_raw.messages and len(final_synth_raw.messages) > 0:
                        last_synth_message = final_synth_raw.messages[-1]
                        if hasattr(last_synth_message, 'role') and hasattr(last_synth_message, 'content'):
                            logger.info(f"[DEBUG TRACE] Last Synthesizer message Role: {last_synth_message.role} Content: {last_synth_message.content[:500]}")
                        elif isinstance(last_synth_message, dict):
                            logger.info(f"[DEBUG TRACE] Last Synthesizer message Role: {last_synth_message.get('role', 'N/A')} Content: {last_synth_message.get('content', 'N/A')[:500]}")
                        else:
                            logger.info(f"[DEBUG TRACE] Last Synthesizer message: {last_synth_message}")

                    # If we have a FinalAnswer object, add the sources to it
                    if isinstance(final_synth_raw.final_output, FinalAnswer):
                        # Add sources from the KB content if available
                        if "kb_sources" in workflow_context:
                            final_synth_raw.final_output.sources_used = workflow_context["kb_sources"]
                            logger.info(f"Added {len(workflow_context['kb_sources'])} sources to the meta-query answer")

                    # Extract the final answer
                    final_markdown_response = extract_final_answer(final_synth_raw)

                    # --- START DEBUG TRACE LOGS (AFTER EXTRACTION) ---
                    logger.info(f"[DEBUG TRACE] extract_final_answer completed.")
                    logger.info(f"[DEBUG TRACE]   final_markdown_response type: {type(final_markdown_response) if final_markdown_response is not None else 'NoneType'}")
                    # Log the actual value of the extracted response
                    logger.info(f"[DEBUG TRACE]   final_markdown_response value: {final_markdown_response}")
                    # --- END DEBUG TRACE LOGS (AFTER EXTRACTION) ---

                    # Log additional details about the extracted response
                    logger.info(f"extract_final_answer output length: {len(final_markdown_response) if isinstance(final_markdown_response, str) else 'N/A'}")
                    logger.info(f"extract_final_answer output preview: {final_markdown_response[:500] if isinstance(final_markdown_response, str) else final_markdown_response}...")

                    # Check if the response is empty or None
                    if not final_markdown_response:
                        logger.warning("[DEBUG TRACE] extract_final_answer returned empty or None response!")
                        # Set a fallback response
                        final_markdown_response = "Sorry, I couldn't generate a response about the knowledge base files. Please try again."
                        logger.info(f"[DEBUG TRACE] Set fallback response: {final_markdown_response}")

                    logger.info("Successfully generated final response via meta-query synthesis.")
                # Note: If synthesizer_success is False, we've already returned a fallback message above

            except Exception as e:
                # Catch potential errors during the synthesis API call
                logger.error(f"Error during final synthesis in meta-query workflow: {e}", exc_info=True)
                # Set a user-friendly error message
                final_markdown_response = "Sorry, I had trouble generating the list of knowledge base files right now. The AI service encountered an issue. Please try again in a moment."

                # If we have specific error types we want to handle differently
                if "502 Bad Gateway" in str(e):
                    final_markdown_response = "Sorry, the AI service is temporarily unavailable (502 Bad Gateway). Please try again in a few moments."
                elif "timeout" in str(e).lower():
                    final_markdown_response = "Sorry, the request timed out while generating the file list. Please try again with a simpler query."

            # Final check to ensure we're not returning None or empty string
            if not final_markdown_response:
                logger.warning("[DEBUG TRACE] Final response is empty or None! Setting fallback response.")
                final_markdown_response = "Sorry, I couldn't retrieve information about the knowledge base files. Please try again later."
                logger.info(f"[DEBUG TRACE] Set final fallback response: {final_markdown_response}")

            # --- START DEBUG TRACE LOGS (BEFORE WORKFLOW RETURN) ---
            logger.info(f"[DEBUG TRACE] KB Meta-Query workflow returning.")
            # Log the actual value being returned by the workflow
            logger.info(f"[DEBUG TRACE]   Returning value: type: {type(final_markdown_response) if final_markdown_response is not None else 'NoneType'} value: {final_markdown_response}")
            # --- END DEBUG TRACE LOGS (BEFORE WORKFLOW RETURN) ---

            # Log additional details about the return value
            logger.info(f"KB meta-query workflow returning response length: {len(final_markdown_response) if isinstance(final_markdown_response, str) else 'N/A'}")
            logger.info(f"KB meta-query workflow returning response preview: {final_markdown_response[:500] if isinstance(final_markdown_response, str) else final_markdown_response}...")

            return final_markdown_response
        elif isinstance(tool_structured_output, dict) and "error" in tool_structured_output:
            # Error from the list tool
            error_msg = tool_structured_output.get("error", "Unknown error")
            logger.warning(f"KB list files failed: {error_msg}")
            return f"I couldn't retrieve the list of files in the knowledge base. Error: {error_msg}"
        else:
            # Unexpected output format
            logger.warning(f"Unexpected output from KB list agent: {type(tool_structured_output)}")
            return "I couldn't retrieve the list of files in the knowledge base due to an unexpected error."
    except Exception as e:
        logger.error(f"KB meta-query workflow failed: {e}", exc_info=True)
        return f"Sorry, an error occurred while retrieving the knowledge base contents: {html.escape(str(e))}"

async def run_kb_search_query(user_query: str, history: List[Dict[str, str]], workflow_context: Dict, vs_id: Optional[str] = None) -> str:
    # Note: vs_id is passed through to maintain API compatibility but not directly used in this function
    """Handles regular search queries to the KB.

    Args:
        user_query: The user's query
        history: The conversation history
        workflow_context: The workflow context containing any necessary information
        vs_id: Optional vector store ID (can be extracted from workflow_context if needed)
    """
    logger.info(f"Running KB Search Query workflow for query: '{user_query[:50]}...'")
    try:
        # Prepare the input for the KB Query Agent
        kb_context = workflow_context.copy()

        # Create a more specific query and document type based on context
        kb_query = user_query
        document_type = "general"  # Default document type

        # Get document analyses from the workflow context if available
        workflow_document_analyses = workflow_context.get("document_analyses", [])

        # Check if we have document analyses to inform our query
        if workflow_document_analyses:
            # Extract document types from analyses
            doc_types = [analysis.doc_type for analysis in workflow_document_analyses if analysis.confidence > 0.7]

            if doc_types:
                # Use the most common document type with high confidence
                from collections import Counter
                most_common_type = Counter(doc_types).most_common(1)[0][0]
                document_type = most_common_type
                logger.info(f"Using document type from analysis: {document_type}")

                # Enhance query with key sections if available
                key_sections = []
                for analysis in workflow_document_analyses:
                    if analysis.key_sections:
                        key_sections.extend(analysis.key_sections)

                if key_sections:
                    # Use up to 3 key sections to enhance the query
                    top_sections = key_sections[:3]
                    kb_query = f"{user_query} related to {', '.join(top_sections)}"
                    logger.info(f"Enhanced query with key sections: {kb_query}")

        # Special case for labor code queries
        if "code de travail" in user_query.lower() or "labor code" in user_query.lower() or "travail" in user_query.lower():
            kb_query = f"Moroccan Labor Code information related to: {user_query}"
            document_type = "code de travail"  # More specific document type

        agent_input = f"Get KB content about '{document_type}' related to: {kb_query}"

        # Run the KB Query Agent to search for content
        kb_data_raw = await Runner.run(
            kb_query_agent,
            input=agent_input,
            context=kb_context
        )
        kb_data = kb_data_raw.final_output

        # Process the search result
        if isinstance(kb_data, RetrievalSuccess):
            # Regular search result
            kb_content = kb_data.content
            kb_sources = kb_data.sources
            logger.info(f"Retrieved KB content for query. Length: {len(kb_content)}")
            # Add sources to workflow context for the synthesizer
            workflow_context["kb_sources"] = kb_sources
        elif isinstance(kb_data, RetrievalError):
            # Error from the search tool
            error_msg = kb_data.error_message
            logger.warning(f"KB search failed: {error_msg}")
            # Provide a clear message that the information couldn't be found
            kb_content = """
            I couldn't find specific information about this topic in the knowledge base.
            The search functionality encountered an error or no relevant content was found.

            Please try:
            - A different query
            - Checking if the relevant files are selected in the knowledge base
            - Consulting official sources for accurate information

            I can only provide information that exists in the knowledge base and will not generate fabricated content.
            """
            logger.info("Using no-results message instead of fabricating information")
        else:
            # Unexpected output format
            logger.warning(f"Unexpected output from KB search agent: {type(kb_data)}")
            kb_content = "I couldn't retrieve information from the knowledge base due to an unexpected error."

        # Create a prompt that includes the query and KB content
        prompt = f"""Answer the following question using ONLY the knowledge base content provided below.\n\nQuestion: {user_query}\n\nIMPORTANT: If the knowledge base content does not contain information to answer this question, clearly state this limitation. DO NOT fabricate or make up information that is not in the provided content. Accuracy is more important than helpfulness."""

        prompt += f"\n\nRelevant Knowledge Base Content:\n{kb_content}"

        synthesis_messages = history + [{
            "role": "user",
            "content": prompt
        }]

        # Log the model being used
        model, _ = get_model_with_fallback()
        logger.info(f"Using model: {model} for search query synthesis")

        # Run the final synthesizer agent with the query and KB content
        try:
            final_synth_raw = await Runner.run(final_synthesizer_agent, input=synthesis_messages, context=workflow_context)

            # If we have a FinalAnswer object, add the sources to it
            if isinstance(final_synth_raw.final_output, FinalAnswer):
                # Add sources from the KB content if available
                if "kb_sources" in workflow_context:
                    final_synth_raw.final_output.sources_used = workflow_context["kb_sources"]
                    logger.info(f"Added {len(workflow_context['kb_sources'])} sources to the final answer")

            # Extract the final answer
            final_markdown_response = extract_final_answer(final_synth_raw)
            return final_markdown_response
        except Exception as agent_error:
            # Check if this is the specific error pattern we're seeing
            error_str = str(agent_error)
            if "markdown_response" in error_str and "title" in error_str and "type" in error_str:
                logger.warning(f"Caught specific error pattern with markdown_response as object. Handling directly.")

                # Try to extract the JSON from the error message
                try:
                    # Find JSON object in the error message
                    start_idx = error_str.find('{')
                    end_idx = error_str.rfind('}')
                    if start_idx >= 0 and end_idx > start_idx:
                        json_str = error_str[start_idx:end_idx+1]
                        data = json.loads(json_str)

                        # Check if it has the specific pattern we're looking for
                        if isinstance(data.get('markdown_response'), dict):
                            md_resp_dict = data['markdown_response']
                            title = md_resp_dict.get('title', '')

                            if title:
                                # Create a reasonable response based on the title
                                if "knowledge base" in title.lower() or "knowledgebase" in title.lower():
                                    return f"# {title}\n\nThe knowledge base is a collection of documents that can be searched for information. You can ask questions about specific topics, and I'll try to find relevant information from the documents in the knowledge base.\n\nCurrently, I don't have detailed information about what specific documents are in the knowledge base. You can try asking questions about specific topics to see if there's relevant information available."
                                else:
                                    return f"# {title}\n\nI don't have detailed information about this topic in the knowledge base. Please try asking a more specific question or check if the relevant documents are included in the knowledge base."
                except Exception as extract_error:
                    logger.error(f"Error extracting from error message: {extract_error}")

                # Fallback response if extraction fails
                return "I don't have detailed information about this topic in the knowledge base. You can try asking specific questions about topics you're interested in, and I'll search for relevant information in the available documents."
            else:
                # Re-raise if it's not the specific error pattern we're handling
                raise
    except Exception as e:
        logger.error(f"KB search query workflow failed: {e}", exc_info=True)
        return f"Sorry, an error occurred while searching the knowledge base: {html.escape(str(e))}"

async def run_standard_agent_rag(user_query: str, history: List[Dict[str, str]], workflow_context: Dict, vs_id: Optional[str] = None) -> str:
    """Implements a simplified RAG workflow using the final_synthesizer_agent.
    This is used as a fallback when no template or temporary files are specified.

    Args:
        user_query: The user's query
        history: The conversation history
        workflow_context: The workflow context containing any necessary information
        vs_id: Optional vector store ID (can be extracted from workflow_context if needed)
    """
    logger.info(f"Running Standard Agent RAG workflow for query: '{user_query[:50]}...'")

    try:
        # Use the new KB workflow function that properly branches based on query type
        return await run_kb_workflow(user_query, history, workflow_context, vs_id)
    except Exception as e:
        logger.error(f"Error in run_standard_agent_rag: {e}", exc_info=True)
        return f"Sorry, an error occurred while processing your request: {html.escape(str(e))}"


# --- Agent Definitions ---
# Define the minimal Data Gathering Agent
data_gathering_agent_minimal = Agent(
    name="DataGatheringAgentMinimal",
    instructions="""Use the provided tools to gather specific data or document content.
    Call the *single* most appropriate tool based on the request.
    Once the tool call is complete, stop and provide the tool's output.""",
    tools=[retrieve_template_content, process_temporary_file, get_kb_document_content],
    model=COMPLETION_MODEL,
    tool_use_behavior="stop_on_first_tool",
    output_type=Union[RetrievalSuccess, RetrievalError]
)

# Define the KB Query Agent that can handle both regular searches and meta-queries
kb_query_agent = Agent(
    name="KBQueryAgent",
    instructions="""You are an agent responsible for retrieving information from the knowledge base.
    You have two tools available:
    1. get_kb_document_content - Use this for regular searches when the user wants information contained within the knowledge base
    2. list_knowledge_base_files - Use this when the user wants to know what's in the knowledge base itself (meta-query)

    DECISION PROCESS:
    - If kb_query_type is "meta" or the user is asking about the contents/structure of the knowledge base itself,
      use the list_knowledge_base_files tool to provide a list of available documents.
    - Otherwise, use the get_kb_document_content tool to search for information within the knowledge base.

    SPECIAL INSTRUCTIONS FOR RETRIEVING CONTENT BY FILE ID:
    - When you receive a message containing "Retrieve the full content from the knowledge base for a specific file" and "FILE DETAILS:",
      you should extract the File ID from the message and use it with the get_kb_document_content tool.
    - Example message format:
      ```
      Retrieve the full content from the knowledge base for a specific file for comparison.

      FILE DETAILS:
      - File ID: file123
      - Filename: document.pdf

      YOUR TASK:
      Use the 'get_kb_document_content' tool to retrieve the full text content for the file specified above.
      Pass the 'File ID' as the 'included_file_ids' parameter to the tool.
      ...
      ```
    - For this input, call get_kb_document_content with:
      * document_type="general"
      * query_or_identifier="retrieve content"
      * included_file_ids=["file123"] (the File ID extracted from the message)

    EXAMPLES:
    - "What does the knowledge base say about labor laws?" → Use get_kb_document_content
    - "What documents are in the knowledge base?" → Use list_knowledge_base_files
    - "Tell me about employment contracts" → Use get_kb_document_content
    - "What files do you have access to?" → Use list_knowledge_base_files
    - Message containing "FILE DETAILS: - File ID: file123" → Use get_kb_document_content with included_file_ids=["file123"]

    Call the appropriate tool based on the query type and provide its output.
    """,
    tools=[get_kb_document_content, list_knowledge_base_files],
    model=COMPLETION_MODEL,
    tool_use_behavior="stop_on_first_tool"
    # No output_type to allow flexibility in return types
)

query_analyzer_agent = Agent(
    name="QueryAnalyzerAgent",
    instructions="""Analyze the user's query, available templates, and temporary files to determine the true intent with high accuracy.

    Possible intents:
    - `kb_query`: User wants information from the knowledge base only
    - `temp_context_query`: User wants information based on temporary files only
    - `kb_query_with_temp_context`: User wants information that combines knowledge base and temporary files
    - `populate_template`: User wants to fill a template with data
    - `analyze_template`: User wants analysis or comparison related to a template, not just filling it

    Guidelines for sophisticated intent determination:
    1. Deeply analyze the semantic meaning of the user's query, not just keywords
    2. Consider the context of any selected template or uploaded files
    3. Understand that templates can be used for both population AND analysis
    4. Recognize that questions about documents may need both KB and temporary file context
    5. Identify when the user is continuing a previous conversation thread

    IMPORTANT: The presence of a selected template in the UI does NOT automatically mean the intent is `populate_template`.
    The user's query is the primary factor in determining intent.

    SPECIAL CASE - KB META-QUERIES:
    When the user is asking about the knowledge base itself (not asking for information contained within it),
    you should still classify this as `kb_query` but add a special field `kb_query_type: "meta"` in the details.

    Examples of KB meta-queries:
    - "What documents are in the knowledge base?"
    - "What's in the KB?"
    - "List the contents of the knowledge base"
    - "Show me what files are available"
    - "What topics does the knowledge base cover?"
    - "What information do you have access to?"

    Examples with nuanced reasoning:
    - "Generate a contract for Omar" → `populate_template` (explicit request to create a document)
    - "How many articles are in the labor code?" → `kb_query` (seeking factual information from KB)
    - "Compare this template with the labor code" → `analyze_template` (requesting comparative analysis)
    - "What does this document say about working hours?" → `temp_context_query` (asking about uploaded content)
    - "Is this contract compliant with labor regulations?" → `kb_query_with_temp_context` (needs both KB and document)
    - "What fields are required in this template?" → `analyze_template` (asking about template structure)
    - "Can you extract the employee details from this document?" → `temp_context_query` (extraction from uploaded file)
    - "What documents are in the knowledge base?" → `kb_query` with `kb_query_type: "meta"` (asking about KB contents)

    For `populate_template` intent:
    - Include a comprehensive list of required fields based on template type and content
    - Consider what fields would be logically needed even if not explicitly mentioned
    - For employment contracts: employee_name, employer_name, start_date, salary, job_title, etc.
    - For invoices: client_name, invoice_date, due_date, items, total_amount, tax_rate, etc.

    For `kb_query_with_temp_context` intent:
    - Include both the specific query topic and the relevant temporary files
    - Explain why both knowledge base and document context are needed

    For `analyze_template` intent:
    - Specify whether knowledge base lookup is needed for the analysis
    - Indicate whether temporary files should be considered in the analysis
    - Include what aspects of the template should be analyzed

    For `kb_query` intent with `kb_query_type: "meta"`:
    - Specify that the user is asking about the knowledge base contents itself
    - Do not include a specific search query as this requires listing files, not searching content

    Output a JSON object with the following structure:
    {
        "intent": "one of the intents listed above",
        "confidence": 0.0-1.0, // How confident you are in this intent determination
        "details": {
            // Intent-specific details as described above
            // Include reasoning for your determination
            // For KB meta-queries, include "kb_query_type": "meta"
        }
    }
    """,
    model=COMPLETION_MODEL
    # No output_type to avoid schema validation issues
)
# Define the minimal DOCX Generation Agent
docx_generation_agent_minimal = Agent(
    name="DOCXGenerationAgentMinimal",
    instructions="""Use the generate_docx_from_markdown tool to convert the provided markdown content into a DOCX file.
    Once the tool call is complete, stop and provide the tool's output.""",
    tools=[generate_docx_from_markdown],
    model=COMPLETION_MODEL,
    tool_use_behavior="stop_on_first_tool",
    output_type=DOCXGenerationResult
)

# Define the minimal Document Analyzer Agent
template_analyzer_agent_minimal = Agent(
    name="TemplateAnalyzerAgentMinimal",
    instructions="""Use the analyze_document_for_workflow tool to analyze the document structure and content.
    Provide a detailed analysis of the document type, structure, and key sections.
    Once the tool call is complete, stop and provide the tool's output.""",
    tools=[analyze_document_for_workflow],
    model=COMPLETION_MODEL,
    tool_use_behavior="stop_on_first_tool",
    output_type=DocumentAnalysis
)
data_extractor_agent = Agent(
    name="DataExtractorAgent",
    instructions="""Extract structured data from the provided context sources.
    You will receive a JSON object with 'context_sources' (array of text) and 'required_fields' (array of field names).

    IMPORTANT GUIDELINES FOR DATA EXTRACTION:
    1. Carefully analyze all context sources to find the required fields
    2. Pay special attention to any KEY: field | VALUE: value patterns in the context
    3. Look for standard patterns like "field: value" or "field - value"
    4. For each required field, extract the most accurate value from the context
    5. If a field is not found in the context, return null for that field
    6. For dates, extract them in their original format
    7. For names, extract the full name as provided
    8. For addresses or locations, extract the complete address
    9. For numerical values, extract them with their units if provided

    Use the extract_data_for_template tool to extract the required fields from the context sources.
    IMPORTANT: After calling the tool, STOP IMMEDIATELY. Do not add any additional text or explanation.

    EXAMPLES OF WHAT TO LOOK FOR:
    - Direct statements: "The employee name is John Smith"
    - Form fields: "Name: John Smith"
    - Formatted data: "KEY: employee_name | VALUE: John Smith"
    - Contextual information: "This contract is between ABC Company and John Smith"

    Be thorough and extract as much information as possible from the provided context.
    """,
    tools=[extract_data_for_template],
    model=COMPLETION_MODEL,
    # Remove output_type to avoid schema validation issues
    tool_use_behavior="stop_on_first_tool"  # Keep tool_use_behavior
)
template_populator_agent = Agent(
    name="TemplatePopulatorAgent",
    instructions="""Efficiently populate templates with provided data. Focus on speed and accuracy.

    CORE TASK:
    1. Fill template placeholders with matching data values
    2. Use [MISSING: field_name] for missing fields
    3. Maintain original template structure
    4. If compliance is mentioned, add BRIEF notes only where critical

    TEMPLATE POPULATION PROCESS:
    - Identify placeholders: [field], {field}, <field>, {{field}}
    - Match with data fields (case-insensitive)
    - Replace placeholders with corresponding values
    - Format dates as DD/MM/YYYY unless template indicates otherwise
    - Format currency with appropriate symbols

    EFFICIENCY GUIDELINES:
    - Focus ONLY on explicit placeholders in the template
    - Do NOT add lengthy explanations or analysis
    - Keep compliance notes brief and targeted
    - Skip non-essential formatting
    - Omit "Additional Information" section unless explicitly requested

    OUTPUT:
    - Return populated template as Markdown
    - Do not include explanations about your process
    """,
    model=COMPLETION_MODEL,
    output_type=FinalAnswer
)
final_synthesizer_agent = Agent(
    name="FinalSynthesizerAgent",
    instructions="""Synthesize final answer from query & context (KB or temp file).

    DETECT the language of the user's query and respond ENTIRELY in that language.

    IMPORTANT RULES:
    1. NEVER fabricate information that is not in the provided context
    2. If the context doesn't contain information to answer the query, clearly state this limitation
    3. Do not try to be helpful by making up information - accuracy is more important than helpfulness
    4. Only provide information that is explicitly supported by the context
    5. If asked about a specific country, language, or document that isn't in the context, clearly state that this information is not available

    SOURCE ATTRIBUTION REQUIREMENTS:
    1. For each piece of information you provide, indicate which source it came from
    2. Use inline citations like [Source 1] or [Source 2] when appropriate
    3. If multiple sources provide the same information, cite all relevant sources
    4. If information comes from a specific section of a document, mention this
    5. Include all sources used in your response

    Follow this structure:
    1. Task understanding: Briefly restate what you're being asked to do
    2. Context summary: Summarize what information is available in the context
    3. Reasoning: Analyze how the context relates to the query
    4. Final answer: Provide a clear, direct answer based ONLY on the context
    5. Limitations: Explicitly state what information was not available in the context

    Special Case - Knowledge Base File Lists:
    When you receive structured data about knowledge base files (usually in JSON format prefaced with "Knowledge Base File List Data (JSON):"), format it into a clear, readable list. For each file:
    - Display the filename prominently
    - Include relevant metadata like creation date and file size
    - Organize the files in a numbered or bulleted list
    - Mention the total number of files at the beginning
    - Explain how the user can use these files (e.g., "You can ask questions about any of these documents")

    Format your response in Markdown.

    IMPORTANT: Your response MUST be a simple string, not a complex object. Do not return a JSON object with title and type fields.
    """,
    model=COMPLETION_MODEL,
    output_type=FinalAnswer
)

# Define a specialized agent for meta-query synthesis (KB file listing)
meta_query_synthesizer_agent = Agent(
    name="MetaQuerySynthesizerAgent",
    instructions="""You are a specialized agent for formatting knowledge base file lists.

    DETECT the language of the user's query and respond ENTIRELY in that language.

    CONTEXT:
    You will receive structured data about knowledge base files (usually in JSON format prefaced with "Knowledge Base File List Data (JSON):").

    YOUR TASK:
    1. Generate a clear, formatted list of these files with their details based on the provided JSON data.
    2. Include the filename, creation date, and size information in a readable format.
    3. Organize the files in a numbered or bulleted list.
    4. Mention the total number of files at the beginning.
    5. Add a brief, helpful explanation of how the user can use these files (e.g., "You can ask questions about any of these documents").

    FORMATTING REQUIREMENTS:
    - Use markdown formatting (headers, bullet points, etc.)
    - Start with a clear header like "# Knowledge Base Contents"
    - List each file with its details in a consistent format
    - Group files by type if appropriate
    - End with the brief explanation

    EXAMPLE OUTPUT FORMAT:
    # Knowledge Base Contents

    The knowledge base contains 5 files:

    ## Documents
    1. **document1.pdf** - Created: 2023-01-15, Size: 1.2 MB
    2. **document2.docx** - Created: 2023-02-20, Size: 850 KB

    ## How to Use the Knowledge Base
    You can ask questions about any of these documents...

    IMPORTANT: Your response MUST be a simple markdown string. Do not include any complex objects or JSON.
    DO NOT enclose your response in triple backticks (```) or any other code block delimiters.
    """,
    model=COMPLETION_MODEL,
    output_type=str  # Set output_type to str for this specialized agent
)

# --- Complex Workflow Orchestration ---
async def run_complex_rag_workflow(user_query: str, vs_id: str, history: List[Dict[str, str]],
                                   temp_files_info: Optional[List[Dict]] = None,
                                   template_to_populate: Optional[str] = None,
                                   chat_id: Optional[str] = None):
    """Orchestrates interaction between agents for complex RAG, including template population."""
    logger.info(f"Running Workflow. Query: '{user_query[:50]}...', TempFiles: {len(temp_files_info or [])}, Template: {template_to_populate}")

    logger.info("Using orchestration engine for workflow execution")
    try:
        # Execute the workflow using the orchestration engine
        return await execute_orchestrated_workflow(
            user_query=user_query,
            vs_id=vs_id,
            history=history,
            temp_files_info=temp_files_info,
            template_to_populate=template_to_populate,
            chat_id=chat_id,
            agent_registry=agent_registry,
            tool_registry=tool_registry
        )
    except Exception as e:
        logger.error(f"Error executing orchestrated workflow: {e}", exc_info=True)
        return f"Error executing workflow: {html.escape(str(e))}"




































    except Exception as workflow_err:
        logger.error(f"Complex Agent workflow failed for VS {vs_id}: {workflow_err}", exc_info=True)
        final_markdown_response = f"Sorry, an error occurred during processing: {html.escape(str(workflow_err))}"

    return final_markdown_response

# --- Modified Chat API Route ---
@app.route('/chat/<chat_id>', methods=['POST'])
async def chat_api(chat_id):
    start_time = time.time()
    if not chat_db: return jsonify({"error": "Database service not available."}), 500

    # Handle both JSON and form data
    user_message = ""
    template_to_populate = None
    temp_files_info = []

    # Check content type to determine how to parse the request
    if request.content_type and 'multipart/form-data' in request.content_type:
        # Handle form data with possible file uploads
        user_message = request.form.get('message', '').strip()
        template_to_populate = request.form.get('template_to_populate')

        # Debug logging
        logger.info(f"Received POST form data - template_to_populate: '{template_to_populate}' (Type: {type(template_to_populate)})")
        logger.info(f"All form data keys: {list(request.form.keys())}")
        logger.info(f"All form data values: {dict(request.form)}")

        # Check if template_to_populate is empty string
        if template_to_populate == '':
            logger.info("template_to_populate is an empty string, setting to None")
            template_to_populate = None

        # Process temporary files if present
        if 'temporary_files[]' in request.files:
            temp_files = request.files.getlist('temporary_files[]')
            for temp_file in temp_files:
                if temp_file and temp_file.filename:
                    # Save the temporary file
                    original_filename = secure_filename(temp_file.filename)
                    temp_filename = str(uuid.uuid4()) + "_" + original_filename
                    temp_path = os.path.join(UPLOAD_FOLDER, temp_filename)
                    try:
                        temp_file.save(temp_path)
                        logger.info(f"Saved temporary file for chat: {original_filename}")
                        temp_files_info.append({
                            'filename': original_filename,
                            'path': temp_path
                        })
                    except Exception as e:
                        logger.error(f"Failed to save temporary file {original_filename}: {e}")
    else:
        # Handle JSON data
        try:
            data = request.get_json()
            if data:
                user_message = data.get('message', '').strip()
                template_to_populate = data.get('template_to_populate')

                # Debug logging
                logger.info(f"Received JSON data - template_to_populate: '{template_to_populate}' (Type: {type(template_to_populate)})")
                logger.info(f"All JSON data keys: {list(data.keys())}")
                logger.info(f"All JSON data values: {data}")

                # Check if template_to_populate is empty string
                if template_to_populate == '':
                    logger.info("template_to_populate is an empty string, setting to None")
                    template_to_populate = None
        except Exception as json_err:
            logger.error(f"Error parsing JSON request: {json_err}")
            return jsonify({"error": "Invalid request format."}), 400

    # Validate message
    if not user_message and not temp_files_info:
        return jsonify({"error": "Message is empty and no files provided."}), 400

    # Get chat details and prepare history
    try:
        chat_details = await asyncio.to_thread(chat_db.get_chat_details, chat_id)
        if not chat_details: return jsonify({"error": "Chat not found."}), 404
        vector_store_id = chat_details.get('vector_store_id')
        if not vector_store_id: logger.error(f"Chat {chat_id} missing VS ID."); return jsonify({"error": "Chat KB link missing."}), 400

        # Add user message to database
        display_message = user_message
        if not display_message and temp_files_info:
            display_message = f"[Uploaded {len(temp_files_info)} file(s): {', '.join(f['filename'] for f in temp_files_info)}]"
        await asyncio.to_thread(chat_db.add_message, chat_id, 'user', display_message)

        # Get message history for context
        message_history_db = await asyncio.to_thread(chat_db.get_messages, chat_id, limit=10)
        history_for_workflow = [{"role": msg["role"], "content": msg["content"]} for msg in message_history_db if msg["role"] != 'user'][-6:]
    except Exception as db_err:
        logger.error(f"DB error pre-processing {chat_id}: {db_err}", exc_info=True)
        # Clean up any temporary files
        for temp_file in temp_files_info:
            try:
                if os.path.exists(temp_file['path']):
                    os.remove(temp_file['path'])
            except Exception as cleanup_err:
                logger.error(f"Failed to clean up temp file: {cleanup_err}")
        return jsonify({"error": "Database error."}), 500

    logger.info(f"Agent Chat API Start: ChatID={chat_id}, VSID={vector_store_id}, Query='{user_message[:50]}...', Files: {len(temp_files_info)}")
    response_content_html = "<p>An error occurred processing your request.</p>" # Default

    try:
        # --- Run the NEW Agent Workflow ---
        logger.info("Calling run_complex_rag_workflow from chat_api endpoint...")
        final_markdown_response = await run_complex_rag_workflow(
            user_query=user_message,
            vs_id=vector_store_id,
            history=history_for_workflow,
            temp_files_info=temp_files_info,
            template_to_populate=template_to_populate,
            chat_id=chat_id
        )

        # Log the response received from the workflow function
        logger.info(f"[DEBUG TRACE] Flask handler received response from workflow: type: {type(final_markdown_response) if final_markdown_response is not None else 'NoneType'} value: {final_markdown_response}")
        logger.info(f"chat_api endpoint received response length: {len(final_markdown_response) if isinstance(final_markdown_response, str) else 'N/A'}")
        logger.info(f"chat_api endpoint received response preview: {final_markdown_response[:500] if isinstance(final_markdown_response, str) else final_markdown_response}...")

        # --- Convert Final Markdown to HTML ---
        try:
            # Log before converting to HTML
            logger.info(f"[DEBUG TRACE] Flask handler preparing to convert markdown to HTML. Value: {final_markdown_response}")
            response_content_html = markdown.markdown(final_markdown_response, extensions=['fenced_code', 'tables', 'nl2br'])
            logger.info(f"Successfully converted markdown to HTML, length: {len(response_content_html)}")
            logger.info(f"[DEBUG TRACE] HTML conversion result: {response_content_html[:200]}...")
        except Exception as md_err:
            logger.error(f"Final Markdown conversion failed: {md_err}");
            response_content_html = f"<p>Error formatting response.</p><pre>{html.escape(final_markdown_response)}</pre>"
        # --- Save Assistant Response ---
        await asyncio.to_thread(chat_db.add_message, chat_id, 'assistant', response_content_html)

    except Exception as workflow_err:
        logger.error(f"Agent workflow failed for chat {chat_id}: {workflow_err}", exc_info=True)
        response_content_html = f"<p>Sorry, an error occurred: {html.escape(str(workflow_err))}</p>"
        try: await asyncio.to_thread(chat_db.add_message, chat_id, 'assistant', response_content_html)
        except Exception as db_final_err: logger.error(f"Failed save final error msg to DB {chat_id}: {db_final_err}")
    finally:
        # Clean up temporary files
        for temp_file in temp_files_info:
            try:
                if os.path.exists(temp_file['path']):
                    os.remove(temp_file['path'])
                    logger.debug(f"Cleaned up temporary file: {temp_file['filename']}")
            except Exception as cleanup_err:
                logger.error(f"Failed to clean up temp file: {cleanup_err}")

    logger.info(f"Agent Chat API End: ChatID={chat_id}. Total time: {time.time() - start_time:.2f}s")
    # Return empty sources for agent workflow for now
    return jsonify({"response": response_content_html, "retrieved_sources": []})
# --- End chat_api ---

# --- Define Custom Trace Processor (Using Correct Base Class & Methods) ---
class PrintTraceProcessor(TracingProcessor): # Inherit from TracingProcessor
    """A simple trace processor that prints trace details using logger."""

    def on_trace_start(self, trace: Trace) -> None:
        # Log only the trace_id which should be available
        logger.debug(f"[TRACE START] ID: {trace.trace_id}")
        # Uncomment to debug available attributes
        # logger.debug(f"[TRACE START] Available trace attributes: {dir(trace)}")

    def on_trace_end(self, trace: Trace) -> None:
        # Avoid accessing potentially non-existent attributes
        error_msg = f", Error: {trace.error}" if hasattr(trace, 'error') and trace.error else ""
        logger.debug(f"[TRACE END] ID: {trace.trace_id}{error_msg}")

    def on_span_start(self, span: Span[Any]) -> None:
        # Safely access span attributes with fallbacks
        span_id = getattr(span, 'span_id', 'unknown')
        parent_id = getattr(span, 'parent_id', 'unknown')
        name = getattr(span, 'name', 'unknown')
        span_type = getattr(span, 'type', 'unknown')

        # Safely handle input
        input_str = "None"
        if hasattr(span, 'input') and span.input:
            try:
                input_str = str(span.input)[:200] + ('...' if len(str(span.input)) > 200 else '')
            except Exception:
                input_str = "<unprintable>"

        logger.debug(f"  [SPAN START] ID: {span_id}, Parent: {parent_id}, Name: {name}, Type: {span_type}, Input: {input_str}")

    def on_span_end(self, span: Span[Any]) -> None:
        # Safely access span attributes with fallbacks
        span_id = getattr(span, 'span_id', 'unknown')
        name = getattr(span, 'name', 'unknown')

        # Safely handle error message
        error_msg = ""
        if hasattr(span, 'error') and span.error:
            error_msg = f", Error: {span.error}"

        # Safely handle output
        output_str = "None"
        if hasattr(span, 'output') and span.output:
            try:
                output_str = str(span.output)[:200] + ('...' if len(str(span.output)) > 200 else '')
            except Exception:
                output_str = "<unprintable>"

        logger.debug(f"  [SPAN END] ID: {span_id}, Name: {name}, Output: {output_str}{error_msg}")

    def shutdown(self) -> None:
        # Called when the application using the SDK is shutting down gracefully.
        logger.debug("[TRACE SHUTDOWN] Trace processor shutting down.")
        # Add any cleanup logic if needed (e.g., flushing buffers to external systems)

    def force_flush(self) -> None:
        # Called to ensure all buffered traces are processed immediately.
        logger.debug("[TRACE FLUSH] Force flush requested.")
        # Add logic to flush buffers if your processor batches data.

# --- End Custom Trace Processor Definition ---

# --- Rename/Delete Chat Routes (Keep as before) ---
@app.route('/rename_chat/<chat_id>', methods=['POST'])
async def rename_chat_route(chat_id):
    if not chat_db:
        flash("DB error.", "error")
        return redirect(url_for('index'))
    new_title = request.form.get('new_title', '').strip()
    if not new_title:
        flash("New title empty.", "error")
    else:
        try:
            success = await asyncio.to_thread(chat_db.rename_chat, chat_id, new_title)
            if success:
                flash(f"Chat renamed.", "success")
            else:
                flash("Rename failed.", "error")
        except Exception as e:
            logger.error(f"Error renaming chat {chat_id}: {e}", exc_info=True)
            flash("Internal error.", "error")
    return redirect(url_for('chat_view', chat_id=chat_id))

@app.route('/delete_chat/<chat_id>', methods=['POST'])
async def delete_chat_route(chat_id):
    if not chat_db:
        flash("DB error.", "error")
        return redirect(url_for('index'))
    try:
        success = await asyncio.to_thread(chat_db.delete_chat, chat_id)
        if success:
            flash("Chat deleted.", "success")
        else:
            flash("Delete failed.", "warning")
    except Exception as e:
        logger.error(f"Error deleting chat {chat_id}: {e}", exc_info=True)
        flash("Internal error.", "error")
    return redirect(url_for('index'))

# --- Flask Routes ---

@app.route('/', methods=['GET'])
async def index():
    # --- Make sure this full implementation is present ---
    db_status_ok = chat_db is not None
    recent_chats = []
    vector_stores = [] # Initialize to prevent potential errors if DB/API fails
    if db_status_ok:
        try: recent_chats = await asyncio.to_thread(chat_db.get_chats, limit=30)
        except Exception as e: logger.error(f"Error fetching recent chats: {e}"); flash("Error loading chats.", "error"); db_status_ok = False
    else: flash("DB error. Chat history disabled.", "error")
    try:
        vector_stores = await get_vector_stores()
    except Exception as e:
        logger.error(f"Error fetching VS list for index: {e}"); flash("Error loading KBs.", "error")

    # Check session for newly created VS
    new_vs_info = session.pop('new_vs_info', None)
    if new_vs_info:
        found = any(vs['id'] == new_vs_info['id'] for vs in vector_stores)
        if not found:
            logger.warning(f"Injecting recently created VS '{new_vs_info['name']}' into list.")
            vector_stores.insert(0, new_vs_info)

    # Render the template, passing None for chat-specific variables for the index page
    return render_template('chat_ui.html',
                           chats=recent_chats,
                           vector_stores=vector_stores,
                           db_status_ok=db_status_ok,
                           current_chat_id=None, # Explicitly None for index
                           current_chat_title='Agent RAG Chat', # Default title
                           current_chat_messages=None, # Explicitly None
                           current_vector_store_id=None) # Explicitly None
    # --- End of index function implementation ---

@app.route('/new_chat', methods=['POST'])
async def new_chat_route():
     # --- Ensure full implementation is here ---
     if not chat_db: flash("DB error.", "error"); return redirect(url_for('index'))
     vector_store_id = request.form.get('vector_store_id', '').strip()
     if not vector_store_id: flash("Select KB.", "error"); return redirect(url_for('index'))
     vs_list = await get_vector_stores(); vs_name = next((vs['name'] for vs in vs_list if vs['id'] == vector_store_id), vector_store_id[-6:])
     try:
        chat_id = await asyncio.to_thread(chat_db.create_chat, vector_store_id, f"Chat: {vs_name}")
        if chat_id: logger.info(f"Created chat {chat_id} for VS {vector_store_id}"); return redirect(url_for('chat_view', chat_id=chat_id))
        else: flash("Failed chat create.", "error"); return redirect(url_for('index'))
     except Exception as e: logger.error(f"Error creating chat in DB: {e}", exc_info=True); flash("Error creating chat.", "error"); return redirect(url_for('index'))
    # --- End of new_chat_route implementation ---

@app.route('/chat_view/<chat_id>', methods=['GET'])
async def chat_view(chat_id):
     # --- Ensure full implementation is here ---
     if not chat_db: flash("DB error.", "error"); return redirect(url_for('index'))
     try:
         chat_details = await asyncio.to_thread(chat_db.get_chat_details, chat_id);
         if not chat_details: flash("Chat not found.", "error"); return redirect(url_for('index'))
         messages = await asyncio.to_thread(chat_db.get_messages, chat_id, limit=200)
         vector_stores = await get_vector_stores(); recent_chats = await asyncio.to_thread(chat_db.get_chats, limit=30)
         return render_template('chat_ui.html', chats=recent_chats, vector_stores=vector_stores,
                                current_chat_id=chat_id, current_chat_title=chat_details.get('title', 'Chat'),
                                current_chat_messages=messages, current_vector_store_id=chat_details.get('vector_store_id'),
                                db_status_ok=True)
     except Exception as e: logger.error(f"Error loading chat view {chat_id}: {e}", exc_info=True); flash("Error loading chat.", "error"); return redirect(url_for('index'))
    # --- End of chat_view implementation ---

# --- DOCX Download Route ---
@app.route('/download_docx/<filename>', methods=['GET'])
def download_docx(filename):
    """Download a generated DOCX file."""
    try:
        # Validate filename to prevent directory traversal attacks
        if not re.match(r'^[\w\-_.]+\.docx$', filename):
            logger.error(f"Invalid DOCX filename requested: {filename}")
            return "Invalid filename", 400

        # Get the file path
        file_path = os.path.join(DOCX_OUTPUT_DIR, filename)

        # Check if the file exists
        if not os.path.exists(file_path):
            logger.error(f"DOCX file not found: {file_path}")
            return "File not found", 404

        # Return the file as an attachment
        return send_file(file_path, as_attachment=True, download_name=filename)
    except Exception as e:
        logger.error(f"Error downloading DOCX file: {e}", exc_info=True)
        return "Error downloading file", 500

# --- File Management API Routes ---
@app.route('/test_file_management', methods=['GET'])
def test_file_management():
    """Test route for file management functionality"""
    return render_template('test_file_management.html')

@app.route('/api/vector_stores', methods=['GET'])
async def vector_stores_api():
    """Get all vector stores"""
    try:
        # Get the OpenAI client
        client = get_openai_client()
        if not client:
            return jsonify({"error": "OpenAI client not available"}), 500

        # Get all vector stores
        vs_response = await asyncio.to_thread(
            client.vector_stores.list
        )

        # Format the response
        vector_stores = []
        for vs in vs_response.data:
            vector_stores.append({
                "id": vs.id,
                "name": vs.name,
                "created_at": vs.created_at
            })

        return jsonify({"vector_stores": vector_stores})
    except Exception as e:
        logger.error(f"Error getting vector stores: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/api/kb_files/<vector_store_id>', methods=['GET'])
async def kb_files_api(vector_store_id):
    """Get files in a knowledge base"""
    try:
        # Get the OpenAI client
        client = get_openai_client()
        if not client:
            return jsonify({"error": "OpenAI client not available"}), 500

        # Get files in the vector store
        files_response = await asyncio.to_thread(
            client.vector_stores.files.list,
            vector_store_id=vector_store_id
        )

        # Format the response
        files = []
        for file in files_response.data:
            # Debug log to see the actual structure
            logger.info(f"Vector store file object attributes: {dir(file)}")

            # Get the file ID - it might be 'id' instead of 'file_id'
            file_id = getattr(file, 'id', None)

            # If we can't find the ID directly, try to extract it from other attributes
            if not file_id:
                # Try to get it from the object representation
                file_str = str(file)
                logger.info(f"File object string representation: {file_str}")

                # Extract ID using regex if possible
                import re
                id_match = re.search(r'id=[\'\"](.+?)[\'\"]', file_str)
                if id_match:
                    file_id = id_match.group(1)
                    logger.info(f"Extracted file ID from string: {file_id}")

            # If we still don't have an ID, use a placeholder
            if not file_id:
                logger.warning(f"Could not determine file ID for file: {file}")
                file_id = f"unknown_{len(files)}"

            # Try to get file details if we have an ID
            file_details = None
            try:
                file_details = await asyncio.to_thread(
                    client.files.retrieve,
                    file_id=file_id
                )
            except Exception as e:
                logger.warning(f"Could not retrieve file details for ID {file_id}: {e}")

            # Build the file info with available data
            file_info = {
                "id": file_id,
                "status": getattr(file, 'status', 'unknown'),
                "metadata": getattr(file, 'metadata', {})
            }

            # Add file details if available
            if file_details:
                file_info.update({
                    "filename": file_details.filename,
                    "created_at": file_details.created_at
                })
            else:
                # Use fallback values
                file_info.update({
                    "filename": getattr(file, 'filename', f"File {len(files) + 1}"),
                    "created_at": getattr(file, 'created_at', None)
                })

            files.append(file_info)

        return jsonify({"files": files})
    except Exception as e:
        logger.error(f"Error getting KB files: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/api/chat_files/<chat_id>', methods=['GET'])
async def get_chat_files_api(chat_id):
    """Get file inclusion settings for a chat"""
    try:
        if not chat_db:
            return jsonify({"error": "Database not available"}), 500

        # Get chat details to verify it exists
        chat_details = await asyncio.to_thread(chat_db.get_chat_details, chat_id)
        if not chat_details:
            return jsonify({"error": "Chat not found"}), 404

        # Get file inclusion settings
        chat_files = await asyncio.to_thread(chat_db.get_chat_files, chat_id)

        # Format the response
        included_file_ids = [file["file_id"] for file in chat_files if file["included"]]
        excluded_file_ids = [file["file_id"] for file in chat_files if not file["included"]]

        return jsonify({
            "included_file_ids": included_file_ids,
            "excluded_file_ids": excluded_file_ids
        })
    except Exception as e:
        logger.error(f"Error getting chat files: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/api/chats', methods=['POST'])
async def create_chat_api():
    """Create a new chat"""
    try:
        if not chat_db:
            return jsonify({"error": "Database not available"}), 500

        # Get request data
        data = request.json
        if not data:
            return jsonify({"error": "No data provided"}), 400

        title = data.get("title", "New Chat")
        vector_store_id = data.get("vector_store_id")

        if not vector_store_id:
            return jsonify({"error": "Vector store ID is required"}), 400

        # Create the chat
        chat_id = await asyncio.to_thread(chat_db.create_chat, title, vector_store_id)

        return jsonify({
            "status": "success",
            "message": "Chat created successfully",
            "chat_id": chat_id
        })
    except Exception as e:
        logger.error(f"Error creating chat: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/api/chat_files/<chat_id>', methods=['POST'])
async def set_chat_files_api(chat_id):
    """Set file inclusion settings for a chat"""
    try:
        if not chat_db:
            return jsonify({"error": "Database not available"}), 500

        # Get chat details to verify it exists
        chat_details = await asyncio.to_thread(chat_db.get_chat_details, chat_id)
        if not chat_details:
            return jsonify({"error": "Chat not found"}), 404

        # Get request data
        data = request.json
        if not data:
            return jsonify({"error": "No data provided"}), 400

        included_file_ids = data.get("included_file_ids", [])
        excluded_file_ids = data.get("excluded_file_ids", [])

        # Update file inclusion settings
        if included_file_ids:
            await asyncio.to_thread(chat_db.set_chat_files, chat_id, included_file_ids, True)
        if excluded_file_ids:
            await asyncio.to_thread(chat_db.set_chat_files, chat_id, excluded_file_ids, False)

        return jsonify({"status": "success", "message": "File inclusion settings updated"})
    except Exception as e:
        logger.error(f"Error setting chat files: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/api/chats/<chat_id>/messages', methods=['POST'])
async def send_chat_message_api(chat_id):
    """Send a message to a chat"""
    try:
        if not chat_db:
            return jsonify({"error": "Database not available"}), 500

        # Get chat details to verify it exists
        chat_details = await asyncio.to_thread(chat_db.get_chat_details, chat_id)
        if not chat_details:
            return jsonify({"error": "Chat not found"}), 404

        # Get request data
        data = request.json
        if not data:
            return jsonify({"error": "No data provided"}), 400

        user_message = data.get("message")
        if not user_message:
            return jsonify({"error": "Message is required"}), 400

        # Get chat history
        chat_history = await asyncio.to_thread(chat_db.get_chat_messages, chat_id)
        history_for_workflow = []
        for msg in chat_history:
            history_for_workflow.append({
                "role": msg["role"],
                "content": msg["content"]
            })

        # Get vector store ID from chat details
        vector_store_id = chat_details["vector_store_id"]

        # Run the workflow
        final_markdown_response = await run_complex_rag_workflow(
            user_query=user_message,
            vs_id=vector_store_id,
            history=history_for_workflow,
            temp_files_info=None,
            template_to_populate=None,
            chat_id=chat_id
        )

        # Save the messages to the database
        await asyncio.to_thread(chat_db.add_message, chat_id, "user", user_message)
        await asyncio.to_thread(chat_db.add_message, chat_id, "assistant", final_markdown_response)

        return jsonify({
            "status": "success",
            "message": "Message sent successfully",
            "response": final_markdown_response
        })
    except Exception as e:
        logger.error(f"Error sending message: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

# --- Template Management Routes ---
@app.route('/upload_template', methods=['POST'])
async def upload_template_route():
    """Handle template file uploads and save them to the template directory."""
    # Check if the file is in the request - handle both 'file' and 'template_files[]' field names
    file = None
    if 'file' in request.files:
        file = request.files['file']
    elif 'template_files[]' in request.files:
        file = request.files['template_files[]']
    else:
        logger.error("No file part in the request")
        return jsonify({"status": "error", "message": "No file part"}), 400

    if file.filename == '':
        logger.error("No file selected")
        return jsonify({"status": "error", "message": "No file selected"}), 400

    # Get metadata - handle both naming conventions
    title = request.form.get('title', '').strip() or request.form.get('template_title', '').strip()
    # Get description but don't use it yet - will be used in future implementation
    _ = request.form.get('description', '').strip() or request.form.get('template_description', '').strip()

    if not title:
        logger.error("No title provided for template")
        return jsonify({"status": "error", "message": "Template title is required"}), 400

    # Check file extension
    filename = secure_filename(file.filename)
    file_ext = os.path.splitext(filename)[1].lower().lstrip('.')

    if file_ext not in ALLOWED_TEMPLATE_EXTENSIONS:
        logger.error(f"Invalid file extension: {file_ext}")
        return jsonify({"status": "error", "message": f"Invalid file extension. Allowed: {', '.join(ALLOWED_TEMPLATE_EXTENSIONS)}"}), 400

    try:
        # Create a filename based on the title
        safe_title = secure_filename(title)
        template_filename = f"{safe_title}.{file_ext}"
        template_path = os.path.join(TEMPLATE_DIR, template_filename)

        # Save the file
        file.save(template_path)
        logger.info(f"Template saved: {template_path}")

        # For PDF files, we might want to extract text for preview/search
        template_text = ""
        if file_ext == 'pdf':
            try:
                # Use our robust PDF text extraction function
                template_text = extract_text_from_pdf(template_path)
                logger.info(f"Extracted {len(template_text)} characters of text from PDF template")
            except Exception as pdf_err:
                logger.warning(f"Could not extract text from PDF template: {pdf_err}")

        # Save metadata (optional - could be stored in a database or JSON file)
        # For now, we'll just return success
        return jsonify({
            "status": "success",
            "message": "Template uploaded successfully",
            "filename": template_filename,
            "title": title,
            "type": file_ext
        })

    except Exception as e:
        logger.error(f"Error saving template: {e}", exc_info=True)
        return jsonify({"status": "error", "message": f"Error saving template: {str(e)}"}), 500

@app.route('/list_templates', methods=['GET'])
async def list_templates_route():
    """List all available templates with their metadata."""
    try:
        templates = []
        for filename in os.listdir(TEMPLATE_DIR):
            file_path = os.path.join(TEMPLATE_DIR, filename)
            if os.path.isfile(file_path):
                file_ext = os.path.splitext(filename)[1].lower().lstrip('.')
                if file_ext in ALLOWED_TEMPLATE_EXTENSIONS:
                    # For now, we'll just use the filename as the title
                    # In a more complete implementation, you'd load metadata from a database
                    title = os.path.splitext(filename)[0].replace('_', ' ').title()
                    templates.append({
                        "filename": filename,
                        "title": title,
                        "description": "",  # Would come from metadata in a full implementation
                        "type": file_ext
                    })

        return jsonify(templates)
    except Exception as e:
        logger.error(f"Error listing templates: {e}", exc_info=True)
        return jsonify({"status": "error", "message": f"Error listing templates: {str(e)}"}), 500

# --- Cache Management Routes ---
@app.route('/cache/clear', methods=['POST'])
async def clear_cache_route():
    """Clear the semantic search cache"""
    try:
        await clear_semantic_cache()
        return jsonify({"status": "success", "message": "Semantic cache cleared"})
    except Exception as e:
        logger.error(f"Error clearing cache: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/cache/stats', methods=['GET'])
async def cache_stats_route():
    """Get cache statistics"""
    try:
        stats = await get_cache_stats()
        return jsonify(stats)
    except Exception as e:
        logger.error(f"Error getting cache stats: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

# --- Other Routes ---
@app.route('/create_vector_store', methods=['POST'])
async def create_vector_store_route():
    global vector_store_cache
    current_client = get_openai_client()
    if not current_client:
        flash("OpenAI client error.", "error")
        return redirect(url_for('index'))

    store_name = request.form.get('store_name', '').strip()
    if not store_name:
        flash("KB name cannot be empty.", "error")
        return redirect(request.referrer or url_for('index'))

    logger.info(f"Attempting to create VS: '{store_name}'")
    try:
        vs = await asyncio.to_thread(current_client.vector_stores.create, name=store_name)
        logger.info(f"Created VS ID: {vs.id}, Name: '{store_name}'")
        new_vs_details = {"id": vs.id, "name": vs.name or f"Store ({vs.id[-6:]})"}
        session['new_vs_info'] = new_vs_details
        logger.info(f"Stored new VS info in session: {new_vs_details}")
        vector_store_cache["list"] = []
        vector_store_cache["last_updated"] = 0
        logger.info("VS cache invalidated after creation.")
        flash(f"KB '{store_name}' created!", "success")

    except (AuthenticationError, APIStatusError) as api_err:
        logger.error(f"Auth/API Error creating VS: {api_err}")
        flash(f"API Error creating KB: {api_err}", "error")
    except Exception as e:
        logger.error(f"Error creating VS: {e}", exc_info=True)
        flash(f"Error creating KB: {str(e)}", "error")

    return redirect(request.referrer or url_for('index'))

@app.route('/upload_to_store', methods=['POST'])
async def upload_to_store_route():
    vector_store_id = request.form.get('vector_store_id')
    if not vector_store_id: flash("Please select KB.", "error"); return redirect(request.referrer or url_for('index'))

    uploaded_files = request.files.getlist('pdf_files[]') # Get the list of FileStorage objects
    if not uploaded_files or all(f.filename == '' for f in uploaded_files): flash("No files selected.", "warning"); return redirect(request.referrer or url_for('index'))

    # --- Parse the metadata JSON string ---
    metadata_json_str = request.form.get('metadata', '{}') # Get the JSON string, default to empty obj
    all_file_metadata = {}
    try:
        all_file_metadata = json.loads(metadata_json_str)
        logger.info(f"Received metadata for files: {all_file_metadata}")
    except json.JSONDecodeError:
        logger.error("Failed to parse metadata JSON from form.")
        flash("Error processing file metadata.", "error")
        return redirect(request.referrer or url_for('index'))
    # --- End Metadata Parsing ---

    saved_files_info = []; upload_errors = False
    # Save files temporarily, keep track of original filename for metadata lookup
    for file in uploaded_files:
        if file and allowed_file(file.filename):
            original_filename = secure_filename(file.filename)
            # Ensure original filename exists in metadata keys if that's how we map
            if original_filename not in all_file_metadata:
                logger.warning(f"Metadata not found for filename '{original_filename}'. Using defaults.")
                # Optionally add a default entry to all_file_metadata here if needed later
                # all_file_metadata[original_filename] = {} # Or skip the file?

            temp_filename = str(uuid.uuid4()) + "_" + original_filename
            temp_path = os.path.join(UPLOAD_FOLDER, temp_filename)
            try: file.save(temp_path); saved_files_info.append((temp_path, original_filename)); logger.info(f"Saved temp file '{original_filename}'.")
            except Exception as e: logger.error(f"Failed save {original_filename}: {e}"); flash(f"Error saving '{original_filename}'.", "error"); upload_errors = True
        elif file.filename != '': flash(f"'{file.filename}' type not allowed.", "error"); upload_errors = True

    if upload_errors: # If saving failed, cleanup and redirect
        for temp_path, _ in saved_files_info:
            try: os.remove(temp_path)
            except OSError as e: logger.warning(f"Could not remove temp file {temp_path}: {e}")
        return redirect(request.referrer or url_for('index'))
    if not saved_files_info: flash("No valid files were processed.", "warning"); return redirect(request.referrer or url_for('index'))

    # --- Call the modified function, passing the metadata dictionary ---
    upload_result = await add_files_to_vector_store(vector_store_id, saved_files_info, all_file_metadata)
    flash(upload_result["message"], upload_result["status"])
    # Temporary files are cleaned up inside add_files_to_vector_store now
    return redirect(request.referrer or url_for('index')) # Redirect back

# --- Custom Runner with Non-Strict Schema Validation ---
class NonStrictRunner(Runner):
    """Custom Runner that uses non-strict schema validation for all agents."""

    @classmethod
    def _get_output_schema(cls, agent: Agent) -> AgentOutputSchemaClass | None:
        """Override to use non-strict schema validation."""
        if agent.output_type is None or agent.output_type is str:
            return None

        return AgentOutputSchemaClass(agent.output_type, strict_json_schema=False)

# Replace the standard Runner with our custom one
Runner = NonStrictRunner

# --- Test Endpoint for Meta-Queries ---
@app.route('/test_meta_query', methods=['GET'])
async def test_meta_query():
    """Test endpoint to trigger a meta-query directly."""
    try:
        # Create a test chat ID
        chat_id = "test_meta_query_" + str(int(time.time()))

        # Create a meta-query
        user_message = "What files are in the knowledge base?"

        # Create minimal history
        history = []

        # Create workflow context
        workflow_context = {
            "chat_id": chat_id
        }

        # Process the message using standard RAG workflow
        logger.info("Calling run_standard_agent_rag from test_meta_query endpoint...")

        # --- START DEBUG TRACE LOGS (IN FLASK HANDLER) ---
        logger.info(f"[DEBUG TRACE] Flask handler calling workflow function.")
        response = await run_standard_agent_rag(user_message, history, workflow_context)

        logger.info(f"[DEBUG TRACE] Flask handler received response from workflow.")
        # Log the actual value received from the workflow
        logger.info(f"[DEBUG TRACE]   Received value: type: {type(response) if response is not None else 'NoneType'} value: {response}")
        # --- END DEBUG TRACE LOGS (IN FLASK HANDLER) ---

        # Log additional details about the response
        logger.info(f"test_meta_query endpoint received response length: {len(response) if isinstance(response, str) else 'N/A'}")
        logger.info(f"test_meta_query endpoint received response preview: {response[:500] if isinstance(response, str) else response}...")

        # --- START DEBUG TRACE LOGS (BEFORE SENDING RESPONSE) ---
        logger.info(f"[DEBUG TRACE] Flask handler preparing to send response to client.")
        # Log the actual value being sent to the client
        logger.info(f"[DEBUG TRACE]   Value: {response}")
        # --- END DEBUG TRACE LOGS (BEFORE SENDING RESPONSE) ---

        return jsonify({
            "query": user_message,
            "response": response
        })
    except Exception as e:
        logger.error(f"Error in test_meta_query endpoint: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500

# --- Import Async Utilities ---
from async_utils import setup_async_for_flask

# --- Main Execution Block ---
if __name__ == '__main__':
    logger.info("Starting Flask application with Agent SDK integration...")

    # Set up asyncio for Flask
    setup_async_for_flask()

    if os.getenv('FLASK_DEBUG') == '1':
        try:
            # Now check for the correctly imported TracingProcessor
            ActualTraceProcessorBase = globals().get('TracingProcessor')
            CustomProcessor = globals().get('PrintTraceProcessor')
            # Ensure base class was imported and custom class inherits from it
            if ActualTraceProcessorBase and CustomProcessor and ActualTraceProcessorBase != object and issubclass(CustomProcessor, ActualTraceProcessorBase):
                 add_trace_processor(CustomProcessor()) # Register instance
                 logger.info("Added PrintTraceProcessor for agent debugging (set logging to DEBUG).")
            else:
                 logger.warning("Trace processor components not correctly defined/imported, skipping registration.")
        except Exception as trace_reg_err:
             logger.error(f"Error during Trace Processor registration: {trace_reg_err}", exc_info=True)

    use_debug = os.getenv('FLASK_DEBUG', '0') == '1'
    logger.info(f"Flask debug mode: {use_debug}")
    app.run(debug=use_debug, host='0.0.0.0', port=5001)